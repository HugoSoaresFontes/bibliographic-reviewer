{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod\n",
    "from urllib.parse import urlencode\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup as bsoup\n",
    "\n",
    "class NCBI_Searcher(metaclass=ABCMeta):\n",
    "    \"\"\" 'Interface' que define a utilização da API das databases da NCBI.\n",
    "    \"\"\"\n",
    "\n",
    "    search_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi'\n",
    "    meta_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi'\n",
    "    fetch_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi'\n",
    "\n",
    "    def search(self, queryterms: list = None, search_type: str = None,\n",
    "               start_year: int = 1900, end_year: int = None,\n",
    "               max_records: int = 20, start_record: int = 0,\n",
    "               author: str = None, journal: str = None):\n",
    "        \"\"\"\n",
    "        Realiza uma pesquisa NCBI.\n",
    "        @param queryterms: list of lists. Terms within the same list are\n",
    "            separated by an OR. Lists are separated by an AND\n",
    "        @param search_type: meta_data or querytext.\n",
    "            meta_data: This field enables a free-text search of all\n",
    "                configured metadata fields and the abstract.\n",
    "            querytext: This field enables a free-text search of all\n",
    "                fields.\n",
    "        @param start_year: Start value of Publication Year to restrict results by.\n",
    "        @param end_year: End value of Publication Year to restrict results by.\n",
    "        @param max_records: The number of records to fetch.\n",
    "        @param start_record: Sequence number of first record to fetch.\n",
    "        @param author: An author's name. Searches both first name and last name\n",
    "            Accepts a list of author names too.\n",
    "        @param journal: An author's name. Accepts a list of journals too.\n",
    "        @return: a dictionaries list whose keys are compatible with Documento model.\n",
    "        \"\"\"\n",
    "\n",
    "        term = self._search_term(queryterms, search_type=search_type)\n",
    "        if author:\n",
    "            author = [author] if type(author) == str else author\n",
    "            func = lambda x, y: \"%s AND %s[Author]\" % (x, y)\n",
    "            term = reduce(func, journal, term)\n",
    "\n",
    "        if journal:\n",
    "            journal = [journal] if type(journal) == str else journal\n",
    "            func = lambda x, y: \"%s AND %s[Journal]\" % (x, y)\n",
    "            term = reduce(func, journal, term)\n",
    "\n",
    "        fixed_payload = {\"retmode\": \"json\", \"datetype\": \"pdat\",\n",
    "                         \"db\": self._db, \"sort\": self._sort_order}\n",
    "        payload = {\"term\": term,\n",
    "                   \"retmax\": max_records, \"retstart\": start_record,\n",
    "                   \"mindate\": start_year, \"maxdate\": end_year or datetime.now().year}\n",
    "        payload.update(fixed_payload)\n",
    "\n",
    "        url = \"%s?%s\" % (self.search_url, urlencode(payload))\n",
    "\n",
    "        print(\"Você pode realizar essa mesma busca no navegador pela com o termo de busca:\\n%s\" % term)\n",
    "\n",
    "        response = requests.get(url).json()['esearchresult']\n",
    "        \n",
    "        print('QTD. resultados: %s' % response['count'])\n",
    "\n",
    "        id_list = response['idlist']\n",
    "\n",
    "        if id_list:\n",
    "            return self._get_article_metadata(*id_list)\n",
    "        return []\n",
    "\n",
    "    def _search_term(self, queryterms: list, search_type: str = None):\n",
    "        \"\"\"Monta o termo de pesquisa completo para mandar para a API.\"\"\"\n",
    "\n",
    "        if search_type in ['querytext', None]:\n",
    "            # Retorna simplesmente a busca concatenando com os OR's e AND's\n",
    "            return \"(%s)\" % \" AND \".join([\"(%s)\" % \" OR \".join(orses) for orses in queryterms])\n",
    "        elif search_type != 'meta_data':\n",
    "            raise Exception('Tipo de pesquisa não faz sentido: %s\\nTipos suportados:' % search_type)\n",
    "\n",
    "        # Retorna concacentando com os OR'S e AND's, mas embutindo também os campos de pesquisa em cada termo\n",
    "        queryterms = [[self._embutir_fields(orses) for orses in andes] for andes in queryterms]\n",
    "        return \"(%s)\" % \" AND \".join([\"(%s)\" % \" OR \".join(orses) for orses in queryterms])\n",
    "\n",
    "    def _embutir_fields(self, term: str):\n",
    "        \"\"\"Faz uma transformação, embutindo fields no termo de pesquisa.\n",
    "        Isso é para poder realizar a pesquisa em apenas alguns campos ao invés de todos.\n",
    "        Exemplo: sendo self.__fields = ['title', 'abstract'],\n",
    "        a chamada\n",
    "        `self._embutir_fields(\"machine learning\")`\n",
    "        Transforma:\n",
    "            machine learning ---> (machine learning[title] OR machine learning[abstract])\n",
    "        \"\"\"\n",
    "\n",
    "        return \"(%s)\" % \" OR \".join([\"%s[%s]\" % (term, field) for field in self._fields])\n",
    "    \n",
    "    @staticmethod\n",
    "    def deepgetter(obj,attrs, default=None):\n",
    "        \"\"\"Faz uma chamada sucessiva da função getattr, para ir pegando os atributos\n",
    "        de um objeto.\n",
    "        Exemplo:\n",
    "        deepgetter(Cidade, 'regiao.pais') é equivalente a fazer Cidade.regiao.pais\n",
    "        \"\"\"\n",
    "        getter = lambda x,y: getattr(x,y, default)\n",
    "        return reduce(getter, attrs.split('.'), obj)\n",
    "        \n",
    "\n",
    "    @abstractmethod\n",
    "    def _get_article_metadata(self, *args):\n",
    "        \"\"\"Cada subclasse deverá implementar a função que pega o retorno da API e transforma numa lista de dicionários\n",
    "        no formato do modelo Documento.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def _fields(self):\n",
    "        \"\"\"Cada subclasse deverá definir quais serão os campos de pesquisa de cada termo.\n",
    "        O retorno deverá ser uma lista de fields.\n",
    "        Exemplo:\n",
    "        return ['title', 'abstract']\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def _db(self):\n",
    "        \"\"\"Cada subclasse deverá definir o seu banco.\n",
    "        Exemplo:\n",
    "        return 'pmc'\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def _sort_order(self):\n",
    "        \"\"\"Cada classe deverá definir o parâmetro sort_order.\n",
    "        Exemplo:\n",
    "        return 'Journal'\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def _article_url(self):\n",
    "        \"\"\"Cada classe deverá definir a URL da página de um artigo.\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class PMC_Searcher(NCBI_Searcher):\n",
    "    \"\"\"Realiza pesquisas na base PMC.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def _fields(self):\n",
    "        return ['Abstract', 'Body - Key Terms', 'MeSH Terms',\n",
    "                'MeSH Major Topic', 'Methods - Key Terms']\n",
    "\n",
    "    @property\n",
    "    def _db(self):\n",
    "        return 'pmc'\n",
    "\n",
    "    @property\n",
    "    def _sort_order(self):\n",
    "        return 'relevance'\n",
    "\n",
    "    @property\n",
    "    def _article_url(self):\n",
    "        return 'https://www.ncbi.nlm.nih.gov/pmc/articles/'\n",
    "\n",
    "    def _get_article_metadata(self, *args):\n",
    "        id_list = ','.join([str(x) for x in args])\n",
    "\n",
    "        payload = {\"id\": id_list, \"db\": self._db, \"retmode\": \"xml\"}\n",
    "        url = \"%s?%s\" % (self.fetch_url, urlencode(payload))\n",
    "        print(url)\n",
    "\n",
    "        soup = bsoup(requests.get(url).content, \"xml\")\n",
    "\n",
    "        pmc_articles = soup.findAll('article')\n",
    "\n",
    "        documentos = []\n",
    "        append = documentos.append\n",
    "\n",
    "        for p_art in pmc_articles:\n",
    "            author_list = p_art.findAll(\"contrib\", {\"contrib-type\": \"author\"})\n",
    "            authors = []\n",
    "            for author in author_list:\n",
    "                try:\n",
    "                    authors.append(\"%s %s\" % (getattr(author, \"given-names\").text, author.surname.text))\n",
    "                except:\n",
    "                    authors.append(author.text)\n",
    "\n",
    "            keywords = [k.text for k in p_art.findAll(\"kwd\")]\n",
    "            pmc_id = soup.findAll(\"article-id\", {\"pub-id-type\": 'pmc'})[0].text\n",
    "\n",
    "            documento = {}\n",
    "            documento['resumo'] = getattr(p_art.abstract, 'text', ' - ')\n",
    "            documento['html_url'] = \"%s%s\" % (self._article_url, pmc_id)\n",
    "            documento['autores'] = \",\".join(authors)\n",
    "            documento['doi'] = p_art.findAll(\"article-id\", {\"pub-id-type\": \"doi\"})[0].text\n",
    "            documento['palavras_chaves'] = \",\".join(keywords)\n",
    "            documento['titulo'] = getattr(p_art, \"article-title\").text\n",
    "            \n",
    "            try:\n",
    "                pub_date = p_art.findAll(\"pub-date\", {\"pub-type\": \"epub\"})[0]\n",
    "            except:\n",
    "                pub_date = p_art.findAll(\"pub-date\", {\"pub-type\": \"ppub\"})[0]\n",
    "                \n",
    "            data_pub_string = \"%s %s\" % (pub_date.year.text, pub_date.month.text)\n",
    "            documento['data'] = datetime.strptime(data_pub_string, \"%Y %m\").date()\n",
    "            \n",
    "            append(documento)\n",
    "\n",
    "        return documentos\n",
    "\n",
    "\n",
    "class PubMed_Searcher(NCBI_Searcher):\n",
    "    \"\"\"Realiza pesquisas na base PubMed.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def _fields(self):\n",
    "        return ['Text Words']\n",
    "\n",
    "    @property\n",
    "    def _db(self):\n",
    "        return 'pubmed'\n",
    "\n",
    "    @property\n",
    "    def _sort_order(self):\n",
    "        return ''\n",
    "\n",
    "    @property\n",
    "    def _article_url(self):\n",
    "        return \"https://www.ncbi.nlm.nih.gov/pubmed/\"\n",
    "\n",
    "    def _get_article_metadata(self, *args):\n",
    "        id_list = ','.join([str(x) for x in args])\n",
    "\n",
    "        payload = {\"id\": id_list, \"db\": self._db, \"retmode\": \"xml\"}\n",
    "        url = \"%s?%s\" % (self.fetch_url, urlencode(payload))\n",
    "\n",
    "        soup = bsoup(requests.get(url).content, \"xml\")\n",
    "\n",
    "        pubmed_articles = soup.findAll('PubmedArticle')\n",
    "\n",
    "        documentos = []\n",
    "        append = documentos.append\n",
    "\n",
    "        for p_art in pubmed_articles:\n",
    "            authors = [\"%s %s\" % (a.ForeName.text, a.LastName.text) for a in p_art.findAll(\"Author\")]\n",
    "            keywords = [k.text for k in p_art.findAll(\"Keyword\")]\n",
    "            global a;\n",
    "            a = p_art\n",
    "            data_pub_string = \"%s %s\" % (p_art.PubDate.Year.text, deepgetter(p_art, 'PubDate.Month.text', default='Jan'))\n",
    "\n",
    "            documento = {}\n",
    "            documento['resumo'] = getattr(p_art.AbstractText, 'text', ' - ')\n",
    "            documento['html_url'] = \"%s%s\" % (self._article_url, p_art.PMID.text)\n",
    "            documento['autores'] = \",\".join(authors)\n",
    "            documento['doi'] = p_art.findAll(\"ArticleId\", {\"IdType\": \"doi\"})[0].text\n",
    "            documento['palavras_chaves'] = \",\".join(keywords)\n",
    "            documento['data'] = datetime.strptime(data_pub_string, \"%Y %b\").date()\n",
    "            documento['titulo'] = p_art.ArticleTitle.text\n",
    "            append(documento)\n",
    "\n",
    "        return documentos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Você pode realizar essa mesma busca no navegador pela com o termo de busca:\n",
      "((cancer)) AND biomedical engineering[Journal]\n",
      "QTD. resultados: 3\n",
      "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?id=4349334%2C4096315%2C5076857&db=pmc&retmode=xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Simplified Warfarin Dose-response Pharmacodynamic ',\n",
       " 'Twenty-fold acceleration of 3D projection reconstr',\n",
       " 'Extended arrays for nonlinear susceptibility magni']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# technology_queryterms = [\n",
    "#     'machine learning', 'deep learning', 'artificial intelligence', \n",
    "#     'neural network', 'scoring system'\n",
    "# ]\n",
    "\n",
    "# health_queryterms = [\n",
    "#     'coronary artery disease', 'chest pain', 'heart disease', 'MACE', \n",
    "#     'Acute Cardiac Complications'\n",
    "# ]\n",
    "\n",
    "# queryterms = [technology_queryterms, health_queryterms]\n",
    "\n",
    "query = [['cancer']]\n",
    "journal = \"biomedical engineering\"\n",
    "\n",
    "r = PMC_Searcher().search(queryterms=query, start_year=2008, max_records=5, journal=journal)\n",
    "[a['titulo'][:50] for a in r]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QTD. resultados: 298\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Body temperature measurement in mice during acute ',\n",
       " 'Classification methods to detect sleep apnea in ad',\n",
       " 'Big Data Analytics, the Microbiome, Host-omic and ',\n",
       " 'Prediction of Incident Hypertension Within the Nex',\n",
       " 'A Clinically-Translatable Machine Learning Algorit']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = PubMed_Searcher().search(queryterms=query, start_year=2016, max_records=5)\n",
    "[(x['titulo'][:50]) for x in r]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
