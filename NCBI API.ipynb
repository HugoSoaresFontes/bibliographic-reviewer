{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod\n",
    "from urllib.parse import urlencode\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup as bsoup\n",
    "\n",
    "class NCBI_Searcher(metaclass=ABCMeta):\n",
    "    \"\"\" 'Interface' que define a utilização da API das databases da NCBI.\n",
    "    \"\"\"\n",
    "\n",
    "    search_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi'\n",
    "    meta_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi'\n",
    "    fetch_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi'\n",
    "\n",
    "    def search(self, queryterms: list = None, search_type: str = None,\n",
    "               start_year: int = 1900, end_year: int = None,\n",
    "               max_records: int = 20, start_record: int = 0,\n",
    "               author: str = None):\n",
    "        \"\"\"\n",
    "        Realiza uma pesquisa NCBI.\n",
    "\n",
    "        @param queryterms: list of lists. Terms within the same list are\n",
    "            separated by an OR. Lists are separated by an AND\n",
    "        @param search_type: meta_data or querytext.\n",
    "            meta_data: This field enables a free-text search of all\n",
    "                configured metadata fields and the abstract.\n",
    "            querytext: This field enables a free-text search of all\n",
    "                fields.\n",
    "        @param start_year: Start value of Publication Year to restrict results by.\n",
    "        @param end_year: End value of Publication Year to restrict results by.\n",
    "        @param max_records: The number of records to fetch.\n",
    "        @param start_record: Sequence number of first record to fetch.\n",
    "        @param author: An author's name. Searches both first name and last name\n",
    "\n",
    "        @return: uma lista de títulos e IDs no formato [(title, id)]\n",
    "        \"\"\"\n",
    "\n",
    "        term = self._search_term(queryterms, search_type=search_type)\n",
    "        if author:\n",
    "            term = \"%s AND %s[Author]\" % (term, author)\n",
    "\n",
    "        fixed_payload = {\"retmode\": \"json\", \"datetype\": \"pdat\",\n",
    "                         \"db\": self._db, \"sort\": self._sort_order}\n",
    "        payload = {\"term\": term,\n",
    "                   \"retmax\": max_records, \"retstart\": start_record,\n",
    "                   \"mindate\": start_year, \"maxdate\": end_year or datetime.now().year}\n",
    "        payload.update(fixed_payload)\n",
    "\n",
    "        url = \"%s?%s\" % (self.search_url, urlencode(payload))\n",
    "        print(\"Url: %s\" % url)\n",
    "\n",
    "        response = requests.get(url).json()['esearchresult']\n",
    "        print('QTD. resultados: %s' % response['count'])\n",
    "        id_list = response['idlist']\n",
    "        if id_list:\n",
    "            return self._get_article_metadata(*id_list)\n",
    "        return []\n",
    "\n",
    "    def _search_term(self, queryterms: list, search_type: str = None):\n",
    "        \"\"\"Monta o termo de pesquisa completo para mandar para a API.\"\"\"\n",
    "\n",
    "        if search_type in ['querytext', None]:\n",
    "            # Retorna simplesmente a busca concatenando com os OR's e AND's\n",
    "            return \"(%s)\" % \" AND \".join([\"(%s)\" % \" OR \".join(orses) for orses in queryterms])\n",
    "        elif search_type != 'meta_data':\n",
    "            raise Exception('Tipo de pesquisa não faz sentido: %s\\nTipos suportados:' % search_type)\n",
    "\n",
    "        # Retorna concacentando com os OR'S e AND's, mas embutindo também os campos de pesquisa em cada termo\n",
    "        queryterms = [[self._embutir_fields(orses) for orses in andes] for andes in queryterms]\n",
    "        return \"(%s)\" % \" AND \".join([\"(%s)\" % \" OR \".join(orses) for orses in queryterms])\n",
    "\n",
    "    def _embutir_fields(self, term: str):\n",
    "        \"\"\"Faz uma transformação, embutindo fields no termo de pesquisa.\n",
    "        Isso é para poder realizar a pesquisa em apenas alguns campos ao invés de todos.\n",
    "\n",
    "        Exemplo: sendo self.__fields = ['title', 'abstract'],\n",
    "        a chamada\n",
    "        `self._embutir_fields(\"machine learning\")`\n",
    "        Transforma:\n",
    "            machine learning ---> (machine learning[title] OR machine learning[abstract])\n",
    "        \"\"\"\n",
    "\n",
    "        return \"(%s)\" % \" OR \".join([\"%s[%s]\" % (term, field) for field in self._fields])\n",
    "\n",
    "    @abstractmethod\n",
    "    def _get_article_metadata(self, *args):\n",
    "        \"\"\"Cada subclasse deverá implementar a função que pega o retorno da API e transforma numa lista de dicionários\n",
    "        no formato do modelo Documento.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def _fields(self):\n",
    "        \"\"\"Cada subclasse deverá definir quais serão os campos de pesquisa de cada termo.\n",
    "        O retorno deverá ser uma lista de fields.\n",
    "        Exemplo:\n",
    "        return ['title', 'abstract']\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def _db(self):\n",
    "        \"\"\"Cada subclasse deverá definir o seu banco.\n",
    "        Exemplo:\n",
    "        return 'pmc'\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def _sort_order(self):\n",
    "        \"\"\"Cada classe deverá definir o parâmetro sort_order.\n",
    "        Exemplo:\n",
    "        return 'Journal'\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def _article_url(self):\n",
    "        \"\"\"Cada classe deverá definir a URL da página de um artigo.\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class PMC_Searcher(NCBI_Searcher):\n",
    "    \"\"\"Realiza pesquisas na base PMC.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def _fields(self):\n",
    "        return ['Abstract', 'Body - Key Terms', 'MeSH Terms',\n",
    "                'MeSH Major Topic', 'Methods - Key Terms']\n",
    "\n",
    "    @property\n",
    "    def _db(self):\n",
    "        return 'pmc'\n",
    "\n",
    "    @property\n",
    "    def _sort_order(self):\n",
    "        return 'relevance'\n",
    "\n",
    "    @property\n",
    "    def _article_url(self):\n",
    "        return 'https://www.ncbi.nlm.nih.gov/pmc/articles/'\n",
    "\n",
    "\n",
    "class PubMed_Searcher(NCBI_Searcher):\n",
    "    \"\"\"Realiza pesquisas na base PubMed.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def _fields(self):\n",
    "        return ['Text Words']\n",
    "\n",
    "    @property\n",
    "    def _db(self):\n",
    "        return 'pubmed'\n",
    "\n",
    "    @property\n",
    "    def _sort_order(self):\n",
    "        return ''\n",
    "\n",
    "    @property\n",
    "    def _article_url(self):\n",
    "        return \"https://www.ncbi.nlm.nih.gov/pubmed/\"\n",
    "\n",
    "    def _get_article_metadata(self, *args):\n",
    "        id_list = ','.join([str(x) for x in args])\n",
    "\n",
    "        payload = {\"id\": id_list, \"db\": self._db, \"retmode\": \"xml\"}\n",
    "        url = \"%s?%s\" % (self.fetch_url, urlencode(payload))\n",
    "        content = requests.get(url).content\n",
    "        soup = bsoup(content, \"xml\")\n",
    "\n",
    "        pubmed_articles = soup.findAll('PubmedArticle')\n",
    "\n",
    "        documentos = []\n",
    "        append = documentos.append\n",
    "        for p_art in pubmed_articles:\n",
    "            authors = [\"%s %s\" % (a.ForeName.text, a.LastName.text) for a in p_art.findAll(\"Author\")]\n",
    "            keywords = [k.text for k in p_art.findAll(\"Keyword\")]\n",
    "            data_pub_string = \"%s %s\" % (p_art.PubDate.Year.text, p_art.PubDate.Month.text)\n",
    "\n",
    "            documento = {}\n",
    "            documento['resumo'] = getattr(p_art.AbstractText, 'text', '')\n",
    "            documento['resumo_url'] = \"%s%s\" % (self._article_url, p_art.PMID.text)\n",
    "            documento['autores'] = \",\".join(authors)\n",
    "            documento['doi'] = p_art.findAll(\"ArticleId\", {\"IdType\": \"doi\"})[0].text\n",
    "            documento['palavras_chaves'] = \",\".join(keywords)\n",
    "            documento['data'] = datetime.strptime(data_pub_string, \"%Y %b\").date()\n",
    "            documento['titulo'] = p_art.ArticleTitle.text\n",
    "            append(documento)\n",
    "\n",
    "        return documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "technology_queryterms = [\n",
    "    'machine learning', 'deep learning', 'artificial intelligence', \n",
    "    'neural network', 'scoring system'\n",
    "]\n",
    "\n",
    "health_queryterms = [\n",
    "    'coronary artery disease', 'chest pain', 'heart disease', 'MACE', \n",
    "    'Acute Cardiac Complications'\n",
    "]\n",
    "\n",
    "# technology_queryterms = [\n",
    "#     'machine learning', 'deep learning'\n",
    "# ]\n",
    "\n",
    "# health_queryterms = [\n",
    "#     'coronary artery disease', 'chest pain'\n",
    "# ]\n",
    "\n",
    "queryterms = [technology_queryterms, health_queryterms]\n",
    "\n",
    "# r = PMC_Searcher().search(queryterms=queryterms, start_year=2008)\n",
    "# [(x[0][:80],x[1]) for x in r[:10]]\n",
    "\n",
    "url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?term=%28%28machine+learning+OR+deep+learning+OR+artificial+intelligence+OR+neural+network+OR+scoring+system%29+AND+%28coronary+artery+disease+OR+chest+pain+OR+heart+disease+OR+MACE+OR+Acute+Cardiac+Complications%29%29&retmax=2&retstart=0&mindate=2008&maxdate=2018&retmode=json&datetype=pdat&db=pubmed&sort=\"\n",
    "requests.get(url).json()['esearchresult']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = PubMed_Searcher().search(queryterms=queryterms, start_year=2008, max_records=2)\n",
    "[(x['titulo'][:50],x[1]) for x in r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "7\n",
      "Url: https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?id=29413172&db=pubmed&retmode=xml\n",
      "b4 parse\n",
      "parsed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'autores': 'Chayakrit Krittanawong',\n",
       "  'data': datetime.date(2018, 3, 1),\n",
       "  'doi': '10.1016/j.hlc.2017.07.012',\n",
       "  'palavras_chaves': 'Big Data Analytics,Big data,Bug-omic,Host-omic,Human microbiome,Machine Learning',\n",
       "  'resumo': '',\n",
       "  'resumo_url': 'https://www.ncbi.nlm.nih.gov/pubmed/29413172',\n",
       "  'titulo': 'Big Data Analytics, the Microbiome, Host-omic and Bug-omic Data and Risk for Cardiovascular Disease.'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PubMed_Searcher()._get_article_metadata(29413172)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
