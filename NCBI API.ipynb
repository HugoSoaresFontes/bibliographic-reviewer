{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod\n",
    "from urllib.parse import urlencode\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup as bsoup\n",
    "\n",
    "class NCBI_Searcher(metaclass=ABCMeta):\n",
    "    \"\"\" 'Interface' que define a utilização da API das databases da NCBI.\n",
    "    \"\"\"\n",
    "    \n",
    "    search_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi'\n",
    "    meta_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi'\n",
    "    fetch_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi'\n",
    "        \n",
    "    def search(self, queryterms: list=None, search_type: str=None,\n",
    "               start_year: int=1900, end_year: int=None,\n",
    "               max_records: int=20, start_record: int=0,\n",
    "               author: str=None):\n",
    "        \"\"\"\n",
    "        Realiza uma pesquisa NCBI.\n",
    "        \n",
    "        @param queryterms: list of lists. Terms within the same list are\n",
    "            separated by an OR. Lists are separated by an AND\n",
    "        @param search_type: meta_data or querytext. \n",
    "            meta_data: This field enables a free-text search of all \n",
    "                configured metadata fields and the abstract.\n",
    "            querytext: This field enables a free-text search of all \n",
    "                fields. \n",
    "        @param start_year: Start value of Publication Year to restrict results by.\n",
    "        @param end_year: End value of Publication Year to restrict results by.\n",
    "        @param max_records: The number of records to fetch.\n",
    "        @param start_record: Sequence number of first record to fetch.\n",
    "        @param author: An author's name. Searches both first name and last name\n",
    "            \n",
    "        @return: uma lista de títulos e IDs no formato [(title, id)]\n",
    "        \"\"\"\n",
    "        \n",
    "        term = self._search_term(queryterms, search_type=search_type)\n",
    "        if author:\n",
    "            term = \"%s AND %s[Author]\" % (term, author)\n",
    "        \n",
    "        fixed_payload = {\"retmode\": \"json\", \"datetype\": \"pdat\",\n",
    "                         \"db\":self._db, \"sort\":self._sort_order}\n",
    "        payload = {\"term\":term, \n",
    "                   \"retmax\": max_records, \"retstart\":start_record,\n",
    "                   \"mindate\": start_year, \"maxdate\": end_year or datetime.now().year}\n",
    "        payload.update(fixed_payload)\n",
    "        \n",
    "        url = \"%s?%s\" % (self.search_url, urlencode(payload))\n",
    "        \n",
    "        response = requests.get(url).json()['esearchresult']\n",
    "        \n",
    "        print('QTD. resultados: %s' % response['count'])\n",
    "        \n",
    "        id_list = response['idlist']\n",
    "\n",
    "        if id_list:\n",
    "            result = self.get_article_metadata(*id_list)['result']\n",
    "\n",
    "            return [(result[uid]['title'], uid) for uid in result['uids']]\n",
    "        return []\n",
    "    \n",
    "    def get_article_metadata(self, *args):\n",
    "        \"\"\"\n",
    "        Retorna os metadados do(s) artigo(s).\n",
    "        \n",
    "        @param args: IDs dos artigos. O ID do artigo depende da base de dados.\n",
    "            O ID de um artigo pode ser diferente entre a PMC e a PubMed\n",
    "            por exemplo. Podem ser passados vários IDs ao mesmo tempo.\n",
    "        @param db: base de dados a ser pesquisada. Se for mais de uma,\n",
    "            pode ser separado por vírgula. As mais úteis serão:\n",
    "            pmc, pubmed\n",
    "        @param retmode: A forma de retorno. None será em XML.\n",
    "        \n",
    "        @return: o json cru retornado pela API.\n",
    "        \"\"\"\n",
    "        id_list = ','.join([str(x) for x in args])\n",
    "            \n",
    "        payload = {\"id\":id_list, \"db\":self._db, \"retmode\":\"json\"}\n",
    "        url = \"%s?%s\" % (self.meta_url, urlencode(payload))\n",
    "        \n",
    "        r = requests.get(url).json()\n",
    "        \n",
    "        return r        \n",
    "    \n",
    "    def _search_term(self, queryterms: list, search_type: str=None):\n",
    "        \"\"\"Monta o termo de pesquisa completo para mandar para a API.\"\"\"\n",
    "        \n",
    "        if search_type in ['querytext', None]:\n",
    "            # Retorna simplesmente a busca concatenando com os OR's e AND's\n",
    "            return \"(%s)\" % \" AND \".join([\"(%s)\" % \" OR \".join(orses)for orses in queryterms])\n",
    "        elif search_type != 'meta_data':\n",
    "            raise Exception('Tipo de pesquisa não faz sentido: %s\\nTipos suportados:' % search_type)\n",
    "        \n",
    "        # Retorna concacentando com os OR'S e AND's, mas embutindo também os campos de pesquisa em cada termo\n",
    "        queryterms = [[self._embutir_fields(orses) for orses in andes] for andes in queryterms]\n",
    "        return \"(%s)\" % \" AND \".join([\"(%s)\" % \" OR \".join(orses)for orses in queryterms])\n",
    "    \n",
    "    def _embutir_fields(self, term: str):\n",
    "        \"\"\"Faz uma transformação, embutindo fields no termo de pesquisa.\n",
    "        Isso é para poder realizar a pesquisa em apenas alguns campos ao invés de todos.\n",
    "        \n",
    "        Exemplo: sendo self.__fields = ['title', 'abstract'],\n",
    "        a chamada \n",
    "        `self._embutir_fields(\"machine learning\")`\n",
    "        Transforma:\n",
    "            machine learning ---> (machine learning[title] OR machine learning[abstract])\n",
    "        \"\"\"\n",
    "        \n",
    "        return \"(%s)\" % \" OR \".join([\"%s[%s]\" % (term, field) for field in self._fields])\n",
    "    \n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def _fields(self):\n",
    "        \"\"\"Cada subclasse deverá definir quais serão os campos de pesquisa de cada termo.\n",
    "        O retorno deverá ser uma lista de fields.\n",
    "        Exemplo: \n",
    "        return ['title', 'abstract']\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def _db(self):\n",
    "        \"\"\"Cada subclasse deverá definir o seu banco.\n",
    "        Exemplo: \n",
    "        return 'pmc'\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def _sort_order(self):\n",
    "        \"\"\"Cada classe deverá definir o parâmetro sort_order.\n",
    "        Exemplo:\n",
    "        return 'Journal'\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def _article_url(self):\n",
    "        \"\"\"Cada classe deverá definir a URL da página de um artigo.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "class PMC_Searcher(NCBI_Searcher):\n",
    "    \"\"\"Realiza pesquisas na base PMC.\"\"\"\n",
    "    \n",
    "    @property\n",
    "    def _fields(self):\n",
    "        return ['Abstract', 'Body - Key Terms', 'MeSH Terms',\n",
    "                'MeSH Major Topic', 'Methods - Key Terms']\n",
    "    \n",
    "    @property\n",
    "    def _db(self):\n",
    "        return 'pmc'\n",
    "    \n",
    "    @property\n",
    "    def _sort_order(self):\n",
    "        return 'relevance'\n",
    "    \n",
    "    @property\n",
    "    def _article_url(self):\n",
    "        return 'https://www.ncbi.nlm.nih.gov/pmc/articles/'\n",
    "    \n",
    "    \n",
    "class PubMed_Searcher(NCBI_Searcher):\n",
    "    \"\"\"Realiza pesquisas na base PubMed.\"\"\"\n",
    "    \n",
    "    @property\n",
    "    def _fields(self):\n",
    "        return ['Text Words']\n",
    "    \n",
    "    @property\n",
    "    def _db(self):\n",
    "        return 'pubmed'\n",
    "    \n",
    "    @property\n",
    "    def _sort_order(self):\n",
    "        return ''\n",
    "    \n",
    "    @property\n",
    "    def _article_url(self):\n",
    "        return \"https://www.ncbi.nlm.nih.gov/pubmed/\"\n",
    "    \n",
    "    def get(self,*args):\n",
    "        id_list = ','.join([str(x) for x in args])\n",
    "        \n",
    "        payload = {\"id\":id_list, \"db\":self._db, \"retmode\":\"xml\"}\n",
    "        url = \"%s?%s\" % (self.fetch_url, urlencode(payload))\n",
    "        \n",
    "        soup = bsoup(requests.get(url).content, \"xml\")\n",
    "        \n",
    "        pubmed_articles = soup.findAll('PubmedArticle')\n",
    "        \n",
    "        documentos = []\n",
    "        append = documentos.append\n",
    "        for p_art in pubmed_articles:\n",
    "            authors = [\"%s %s\" % (a.ForeName.text, a.LastName.text) for a in p_art.findAll(\"Author\")]\n",
    "            keywords = [k.text for k in p_art.findAll(\"Keyword\")]\n",
    "            data_pub_string = \"%s %s\" % (p_art.PubDate.Year.text, p_art.PubDate.Month.text)\n",
    "            \n",
    "            documento = {}\n",
    "            documento['resumo'] = getattr(p_art.AbstractText, 'text', '')\n",
    "            documento['resumo_url'] = \"%s%s\" % (self._article_url, p_art.PMID.text)\n",
    "            documento['autores'] = \",\".join(authors)\n",
    "            documento['doi'] = p_art.findAll(\"ArticleId\", {\"IdType\":\"doi\"})[0].text\n",
    "            documento['palavras_chaves'] = \",\".join(keywords)\n",
    "            documento['data_publicacao'] = datetime.strptime(data_pub_string, \"%Y %b\")\n",
    "            documento['titulo_publicacao'] = p_art.Title.text\n",
    "            documento['titulo'] = p_art.ArticleTitle.text\n",
    "            append(documento)\n",
    "        \n",
    "        return documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QTD. resultados: 72277\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ESICM LIVES 2016: part one: Mi', '5042924'),\n",
       " ('36th International Symposium o', '5493079'),\n",
       " ('Abstracts from the 36th Annual', '3654146'),\n",
       " ('Abstracts from the 38th Annual', '4405523'),\n",
       " ('Abstracts from the 37th Annual', '4429500'),\n",
       " ('25th Annual Computational Neur', '5001212'),\n",
       " ('ESICM LIVES 2016: part three: ', '5042925'),\n",
       " ('Heart Disease and Stroke Stati', '5408160'),\n",
       " ('ESICM LIVES 2016: part two: Mi', '5042923'),\n",
       " ('Korean Guidelines for the Appr', '4347263')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "technology_queryterms = [\n",
    "    'machine learning', 'deep learning', 'artificial intelligence', \n",
    "    'neural network', 'scoring system'\n",
    "]\n",
    "\n",
    "health_queryterms = [\n",
    "    'coronary artery disease', 'chest pain', 'heart disease', 'MACE', \n",
    "    'Acute Cardiac Complications'\n",
    "]\n",
    "\n",
    "# technology_queryterms = [\n",
    "#     'machine learning', 'deep learning'\n",
    "# ]\n",
    "\n",
    "# health_queryterms = [\n",
    "#     'coronary artery disease', 'chest pain'\n",
    "# ]\n",
    "\n",
    "queryterms = [technology_queryterms, health_queryterms]\n",
    "\n",
    "r = PMC_Searcher().search(queryterms=queryterms, start_year=2008)\n",
    "[(x[0][:80],x[1]) for x in r[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QTD. resultados: 2675\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Cytokine Responses to Rhinovirus and Development of Asthma, Allergic Sensitizati',\n",
       "  '29466680'),\n",
       " ('Comparison of fast multi-slice and standard segmented techniques for detection o',\n",
       "  '29458430'),\n",
       " ('Do patients with diabetes type 2 or chronic heart failure understand a medicatio',\n",
       "  '29441963'),\n",
       " ('Development and preliminary testing of the Brief Developmental Assessment: an ea',\n",
       "  '29433600'),\n",
       " ('Classification of the clinical images for benign and malignant cutaneous tumors ',\n",
       "  '29428356'),\n",
       " ('Controlling the Risk Domain in Pediatric Asthma through Personalized Care.',\n",
       "  '29427984'),\n",
       " ('Scoring system to guide decision making for the use of bilateral internal mammar',\n",
       "  '29413876'),\n",
       " ('Big Data Analytics, the Microbiome, Host-omic and Bug-omic Data and Risk for Car',\n",
       "  '29413172'),\n",
       " ('Automatic Calcium Scoring in Low-Dose Chest CT Using Deep Neural Networks With D',\n",
       "  '29408789'),\n",
       " ('Predicting cardiogenic pulmonary edema in heart failure patients by using an N-t',\n",
       "  '29408167')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = PubMed_Searcher().search(queryterms=queryterms, start_year=2008)\n",
    "[(x[0][:80],x[1]) for x in r[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml = requests.get(\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=5042924,5493079&retmode=xml\")\n",
    "soup = bsoup(xml.content, \"xml\").findAll('article')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pubmed = requests.get(\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=29413172&retmode=xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bsoup(pubmed.content, \"xml\").findAll('PubmedArticle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'autores': 'Chayakrit Krittanawong',\n",
       "  'data_publicacao': datetime.datetime(2018, 3, 1, 0, 0),\n",
       "  'doi': '10.1016/j.hlc.2017.07.012',\n",
       "  'palavras_chaves': 'Big Data Analytics,Big data,Bug-omic,Host-omic,Human microbiome,Machine Learning',\n",
       "  'resumo': '',\n",
       "  'resumo_url': 'https://www.ncbi.nlm.nih.gov/pubmed/29413172',\n",
       "  'titulo': 'Big Data Analytics, the Microbiome, Host-omic and Bug-omic Data and Risk for Cardiovascular Disease.',\n",
       "  'titulo_publicacao': 'Heart, lung & circulation'},\n",
       " {'autores': 'Nikolas Lessmann,Bram van Ginneken,Majd Zreik,Pim A de Jong,Bob D de Vos,Max A Viergever,Ivana Isgum',\n",
       "  'data_publicacao': datetime.datetime(2018, 2, 1, 0, 0),\n",
       "  'doi': '10.1109/TMI.2017.2769839',\n",
       "  'palavras_chaves': '',\n",
       "  'resumo': 'Heavy smokers undergoing screening with low-dose chest CT are affected by cardiovascular disease as much as by lung cancer. Low-dose chest CT scans acquired in screening enable quantification of atherosclerotic calcifications and thus enable identification of subjects at increased cardiovascular risk. This paper presents a method for automatic detection of coronary artery, thoracic aorta, and cardiac valve calcifications in low-dose chest CT using two consecutive convolutional neural networks. The first network identifies and labels potential calcifications according to their anatomical location and the second network identifies true calcifications among the detected candidates. This method was trained and evaluated on a set of 1744 CT scans from the National Lung Screening Trial. To determine whether any reconstruction or only images reconstructed with soft tissue filters can be used for calcification detection, we evaluated the method on soft and medium/sharp filter reconstructions separately. On soft filter reconstructions, the method achieved F1scores of 0.89, 0.89, 0.67, and 0.55 for coronary artery, thoracic aorta, aortic valve, and mitral valve calcifications, respectively. On sharp filter reconstructions, the F1scores were 0.84, 0.81, 0.64, and 0.66, respectively. Linearly weighted kappa coefficients for risk category assignment based on per subject coronary artery calcium were 0.91 and 0.90 for soft and sharp filter reconstructions, respectively. These results demonstrate that the presented method enables reliable automatic cardiovascular risk assessment in all low-dose chest CT scans acquired for lung cancer screening.',\n",
       "  'resumo_url': 'https://www.ncbi.nlm.nih.gov/pubmed/29408789',\n",
       "  'titulo': 'Automatic Calcium Scoring in Low-Dose Chest CT Using Deep Neural Networks With Dilated Convolutions.',\n",
       "  'titulo_publicacao': 'IEEE transactions on medical imaging'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PubMed_Searcher().get(29413172,29408789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ForeName>Kevin</ForeName>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup[0].findAll(\"Author\")[0].ForeName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Big Data Analytics',\n",
       " 'Big data',\n",
       " 'Bug-omic',\n",
       " 'Host-omic',\n",
       " 'Human microbiome',\n",
       " 'Machine Learning']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[k.text for k in soup[0].findAll(\"Keyword\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Automatic prediction of coronary artery disease from clinical narratives.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup[0].ArticleTitle.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
