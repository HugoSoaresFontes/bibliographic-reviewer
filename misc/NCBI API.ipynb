{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod\n",
    "from urllib.parse import urlencode\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup as bsoup\n",
    "from functools import reduce\n",
    "import time\n",
    "import xmltodict, json\n",
    "import ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "\n",
    "class ThreadWithReturnValue(Thread):\n",
    "    def __init__(self, group=None, target=None, name=None,\n",
    "                 args=(), kwargs=None, daemon=None):\n",
    "        Thread.__init__(self, group=group, target=target, name=name,\n",
    "                 args=args, kwargs=kwargs, daemon=daemon)\n",
    "        self._return = None\n",
    "    def run(self):\n",
    "        if self._target is not None:\n",
    "            self._return = self._target(*self._args,\n",
    "                                                **self._kwargs)\n",
    "    def join(self):\n",
    "        Thread.join(self)\n",
    "        return self._return\n",
    "\n",
    "    \n",
    "class NCBI_Searcher(metaclass=ABCMeta):\n",
    "    \"\"\" 'Interface' que define a utilização da API das databases da NCBI.\n",
    "    \"\"\"\n",
    "\n",
    "    search_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi'\n",
    "    meta_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi'\n",
    "    fetch_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi'\n",
    "    ncbi_register = {\"tool\":\"Atena\", \"email\":\"ddddiegolima@gmail.com\"}\n",
    "    max_pagination = 20\n",
    "    recursive = True\n",
    "\n",
    "    def search(self, queryterms: list = None, search_type: str = None,\n",
    "               start_year: int = None, end_year: int = None,\n",
    "               max_records: int = None, start_record: int = None,\n",
    "               author: str = None, journal: str = None, search_url: str = None):\n",
    "        \"\"\"\n",
    "        Realiza uma pesquisa NCBI.\n",
    "        @param queryterms: list of lists. Terms within the same list are\n",
    "            separated by an OR. Lists are separated by an AND\n",
    "        @param search_type: meta_data or querytext.\n",
    "            meta_data: This field enables a free-text search of all\n",
    "                configured metadata fields and the abstract.\n",
    "            querytext: This field enables a free-text search of all\n",
    "                fields.\n",
    "        @param start_year: Start value of Publication Year to restrict results by.\n",
    "        @param end_year: End value of Publication Year to restrict results by.\n",
    "        @param max_records: The number of records to fetch.\n",
    "        @param start_record: Sequence number of first record to fetch.\n",
    "        @param author: An author's name. Searches both first name and last name\n",
    "            Accepts a list of author names too.\n",
    "        @param journal: An author's name. Accepts a list of journals too.\n",
    "        @param search_url: Optionally you can directly specify the URL to \n",
    "            query from. Setting this parameter will ignore the other parameters.\n",
    "        @return: a dictionaries list whose keys are compatible with Documento model.\n",
    "        \"\"\"\n",
    "\n",
    "        term = self._search_term(queryterms, search_type=search_type)\n",
    "        if author:\n",
    "            author = [author] if type(author) == str else author\n",
    "            author = ['%s[Author]' % a for a in author]\n",
    "            term = \"%s AND (%s)\" % (term, \" OR \".join(author) )\n",
    "\n",
    "        if journal:\n",
    "            journal = [journal] if type(journal) == str else journal\n",
    "            journal = ['\"%s\"[Journal]' % j for j in journal]\n",
    "            term = \"%s AND (%s)\" % (term, \" OR \".join(journal) )\n",
    "\n",
    "        fixed_payload = {\"retmode\": \"json\", \"datetype\": \"pdat\",\n",
    "                         \"db\": self._db, \"sort\": self._sort_order}\n",
    "        payload = {\"term\": term,\n",
    "                   \"retmax\": max_records or '', \"retstart\": start_record or '',\n",
    "                   \"mindate\": start_year or '', \"maxdate\": end_year or ''}\n",
    "        payload.update(fixed_payload)\n",
    "        payload.update(self.ncbi_register)\n",
    "        url = search_url if search_url else \"%s?%s\" % (self.search_url, urlencode(payload))\n",
    "        \n",
    "#         print(\"URL SEARCH: %s\" % url)\n",
    "        t_00 = time.time()\n",
    "        response = requests.get(url).json()['esearchresult']\n",
    "        print('{:15s}{:6.3f}'.format(\"response\",time.time() - t_00))\n",
    "        quantidade_artigos = int(response['count'])\n",
    "        if self.recursive:\n",
    "            print(\"Artigos encontrados: \",quantidade_artigos)\n",
    "        # Se o usuário não limitou quantidade de resultados, então traz tudo\n",
    "        max_records = max_records or quantidade_artigos\n",
    "        \n",
    "        retorno = []\n",
    "        \n",
    "        ###\n",
    "        ### BLOCO FANTASMA\n",
    "        ###\n",
    "        # Se houver necessidade de paginação...\n",
    "        if quantidade_artigos > self.max_pagination and max_records > self.max_pagination:\n",
    "            # self.recursive só sera True se a chamada estiver sendo feita pelo usuário.\n",
    "            # Isso serve para garantir que cada chamada da função self.search\n",
    "            # neste bloco só acontecerá com um nível de recursividade.\n",
    "            if self.recursive:\n",
    "                self.recursive = False\n",
    "                \n",
    "                threads = []\n",
    "                payload.update({'retmax':self.max_pagination})\n",
    "                for i,x in enumerate(range(20, quantidade_artigos+1, self.max_pagination)):\n",
    "                    \n",
    "                    payload.update({'retstart':x})\n",
    "                    kwargs = {\"search_url\": \"%s?%s\" % (self.search_url, urlencode(payload))}\n",
    "                    \n",
    "                    thread = ThreadWithReturnValue(target=self.search, kwargs=kwargs)\n",
    "                    threads.append(thread)\n",
    "                    thread.start()\n",
    "                    \n",
    "                    if (i+1)%3==0:\n",
    "                        print(\"sleeping\")\n",
    "                        time.sleep(2)\n",
    "\n",
    "                for thread in threads:\n",
    "                    lista = thread.join()\n",
    "                    print(\"thread fetching \",len(lista))\n",
    "                    retorno.extend(lista)\n",
    "                    \n",
    "                self.recursive = True\n",
    "                \n",
    "        ###\n",
    "        ### FIM BLOCO FANTASMA\n",
    "        ###\n",
    "                    \n",
    "\n",
    "        id_list = response['idlist']\n",
    "\n",
    "        if id_list:\n",
    "            lista = self._get_article_metadata(*id_list)\n",
    "            retorno.extend(lista)\n",
    "        return retorno\n",
    "\n",
    "    def _search_term(self, queryterms: list, search_type: str = None):\n",
    "        \"\"\"Monta o termo de pesquisa completo para mandar para a API.\"\"\"\n",
    "        \n",
    "        if type(queryterms) != list:\n",
    "            return\n",
    "\n",
    "        if search_type in ['querytext', None]:\n",
    "            # Retorna simplesmente a busca concatenando com os OR's e AND's\n",
    "            return \"(%s)\" % \" AND \".join([\"(%s)\" % \" OR \".join(orses) for orses in queryterms])\n",
    "        elif search_type != 'meta_data':\n",
    "            raise Exception('Tipo de pesquisa não faz sentido: %s\\nTipos suportados:' % search_type)\n",
    "\n",
    "        # Retorna concacentando com os OR'S e AND's, mas embutindo também os campos de pesquisa em cada termo\n",
    "        queryterms = [[self._embutir_fields(orses) for orses in andes] for andes in queryterms]\n",
    "        return \"(%s)\" % \" AND \".join([\"(%s)\" % \" OR \".join(orses) for orses in queryterms])\n",
    "\n",
    "    def _embutir_fields(self, term: str):\n",
    "        \"\"\"Faz uma transformação, embutindo fields no termo de pesquisa.\n",
    "        Isso é para poder realizar a pesquisa em apenas alguns campos ao invés de todos.\n",
    "        Exemplo: sendo self.__fields = ['title', 'abstract'],\n",
    "        a chamada\n",
    "        `self._embutir_fields(\"machine learning\")`\n",
    "        Transforma:\n",
    "            machine learning ---> (machine learning[title] OR machine learning[abstract])\n",
    "        \"\"\"\n",
    "\n",
    "        return \"(%s)\" % \" OR \".join([\"%s[%s]\" % (term, field) for field in self._fields])\n",
    "\n",
    "    @staticmethod\n",
    "    def deepgetter(obj, attrs, default=None):\n",
    "        \"\"\"Faz uma chamada sucessiva da função getattr, para ir pegando os atributos\n",
    "        de um objeto.\n",
    "        Exemplo:\n",
    "        deepgetter(Cidade, 'regiao.pais') é equivalente a fazer Cidade.regiao.pais\n",
    "        \"\"\"\n",
    "        getter = lambda x, y: getattr(x, y, default)\n",
    "        return reduce(getter, attrs.split('.'), obj)\n",
    "\n",
    "    @abstractmethod\n",
    "    def _get_article_metadata(self, *args):\n",
    "        \"\"\"Cada subclasse deverá implementar a função que pega o retorno da API e transforma numa lista de dicionários\n",
    "        no formato do modelo Documento.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def _fields(self):\n",
    "        \"\"\"Cada subclasse deverá definir quais serão os campos de pesquisa de cada termo.\n",
    "        O retorno deverá ser uma lista de fields.\n",
    "        Exemplo:\n",
    "        return ['title', 'abstract']\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def _db(self):\n",
    "        \"\"\"Cada subclasse deverá definir o seu banco.\n",
    "        Exemplo:\n",
    "        return 'pmc'\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def _sort_order(self):\n",
    "        \"\"\"Cada classe deverá definir o parâmetro sort_order.\n",
    "        Exemplo:\n",
    "        return 'Journal'\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def _article_url(self):\n",
    "        \"\"\"Cada classe deverá definir a URL da página de um artigo.\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class PMC_Searcher(NCBI_Searcher):\n",
    "    \"\"\"Realiza pesquisas na base PMC.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def _fields(self):\n",
    "        return ['Abstract', 'Body - Key Terms', 'MeSH Terms',\n",
    "                'MeSH Major Topic', 'Methods - Key Terms']\n",
    "\n",
    "    @property\n",
    "    def _db(self):\n",
    "        return 'pmc'\n",
    "\n",
    "    @property\n",
    "    def _sort_order(self):\n",
    "        return 'relevance'\n",
    "\n",
    "    @property\n",
    "    def _article_url(self):\n",
    "        return 'https://www.ncbi.nlm.nih.gov/pmc/articles/'\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_data(p_art):\n",
    "        \"\"\"Vasculha o XML (um <PubmedArticle>) para encontrar a data de publicação\n",
    "        Se for encontrada uma data válida, retorna um datetime.\n",
    "        Se não, retorna uma string, que espera-se que contenha uma informação de data\"\"\"\n",
    "\n",
    "        try:\n",
    "            pub_date = p_art.findAll(\"pub-date\", {\"pub-type\": \"epub\"})[0]\n",
    "        except:\n",
    "            pub_date = p_art.findAll(\"pub-date\", {\"pub-type\": \"ppub\"})[0]\n",
    "\n",
    "        data_pub_string = \"%s %s\" % (pub_date.year.text, NCBI_Searcher.deepgetter(pub_date, 'month.text', default='Jan'))\n",
    "\n",
    "        try:\n",
    "            data = datetime.strptime(data_pub_string, \"%Y %m\").date()\n",
    "        except:\n",
    "            try:\n",
    "                data = datetime.strptime(data_pub_string, \"%Y %b\").date()\n",
    "            except:\n",
    "                data = data_pub_string\n",
    "\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_unique_id(p_art):\n",
    "        \"\"\"Vascula o XML (um <PubmedArticle>) para encontrar o ID único do artigo.\n",
    "        Se nao tiver DOI presente no XML, coloca o ID que tiver (esperado que seja o PubMed ID)\"\"\"\n",
    "\n",
    "        try:\n",
    "            unique_id = p_art.findAll(\"article-id\", {\"pub-id-type\": \"doi\"})[0].text\n",
    "        except:\n",
    "            unique_id = p_art.findAll(\"article-id\")[0]\n",
    "            unique_id = \"%s%s\" % (unique_id['pub-id-type'], unique_id.text)\n",
    "\n",
    "        return unique_id\n",
    "\n",
    "    def _get_article_metadata(self, *args):\n",
    "        id_list = ','.join([str(x) for x in args])\n",
    "\n",
    "        payload = {\"id\": id_list, \"db\": self._db, \"retmode\": \"xml\"}\n",
    "        payload.update(self.ncbi_register)\n",
    "        url = \"%s?%s\" % (self.fetch_url, urlencode(payload))\n",
    "        print(\"URL META: %s\" % url)\n",
    "\n",
    "        t_05 = time.time()\n",
    "        r = requests.get(url)\n",
    "        print('{:15s}{:6.3f}'.format(\"response_M\",time.time() - t_05))\n",
    "        \n",
    "        t_02 = time.time()\n",
    "        # Pegar o XML, e transformar num dicionário\n",
    "        d = json.loads(json.dumps(xmltodict.parse(r.content)))\n",
    "        articles = d['pmc-articleset']['article']\n",
    "        print('{:15s}{:6.3f}'.format(\"parse\",time.time() - t_02))\n",
    "\n",
    "        documentos = []\n",
    "        append = documentos.append\n",
    "\n",
    "        t_04 = time.time()\n",
    "        debug = False\n",
    "        \n",
    "        ###\n",
    "        ### DISCLAIMER: o código abaixo foi sendo feito aos ajustes para cada erro que dava\n",
    "        ### não tente ler!\n",
    "        ###\n",
    "        \n",
    "        for article in articles:\n",
    "            ### TITULO\n",
    "            try:\n",
    "                title = article['front']['article-meta']['title-group']['article-title']\n",
    "                if type(title) == dict:\n",
    "                    # Pegando, dentre as possibilidades, a maior string (com sorte, esse realmente será o titulo)\n",
    "                    title = sorted(title.values(), key=len)[-1]\n",
    "            except Exception as e:\n",
    "                title = ''\n",
    "                if debug:\n",
    "                    print(e.__class__.__name__, e, 'title')\n",
    "                    ipdb.set_trace()\n",
    "\n",
    "            ### AUTORES\n",
    "            try:\n",
    "                authors = []\n",
    "                try:\n",
    "                    for contrib in article['front']['article-meta']['contrib-group']['contrib']:\n",
    "                        try:\n",
    "                            authors.append(\"%s %s\" % (contrib['name']['given-names'], contrib['name']['surname']))\n",
    "                        except:\n",
    "                            pass\n",
    "                except TypeError:\n",
    "                    for contrib in article['front']['article-meta']['contrib-group'][0]['contrib']:\n",
    "                        try:\n",
    "                            authors.append(\"%s %s\" % (contrib['name']['given-names'], contrib['name']['surname']))\n",
    "                        except:\n",
    "                            pass\n",
    "                    pass\n",
    "            except Exception as e:\n",
    "                authors = []\n",
    "                if debug:\n",
    "                    print(e.__class__.__name__, e, 'authors')\n",
    "                    ipdb.set_trace()\n",
    "\n",
    "            ### PALAVRAS CHAVE\n",
    "            try:\n",
    "                palavras_chave = [k if type(k) == str else k['#text'] for k in\n",
    "                                  article['front']['article-meta']['kwd-group']['kwd']]\n",
    "            except Exception as e:\n",
    "                palavras_chave = []\n",
    "                if debug:\n",
    "                    print(e.__class__.__name__, e, 'kwd')\n",
    "                    ipdb.set_trace()\n",
    "\n",
    "            ### DOI / IDs\n",
    "            try:\n",
    "                doi = [id['#text'] for id in article['front']['article-meta']['article-id'] if id['@pub-id-type'] == 'doi'][-1] or ''\n",
    "            except Exception as e:\n",
    "                doi = ''\n",
    "                if debug:\n",
    "                    print(e.__class__.__name__, e, 'doi')\n",
    "                    ipdb.set_trace()\n",
    "\n",
    "            try:\n",
    "                pmc_id = [id['#text'] for id in article['front']['article-meta']['article-id'] if id['@pub-id-type'] == 'pmc'][-1] or ''\n",
    "            except Exception as e:\n",
    "                pmc_id = ''\n",
    "                if debug:\n",
    "                    print(e.__class__.__name__, e, 'pmc_id')\n",
    "                    ipdb.set_trace()\n",
    "\n",
    "            ### Abstract\n",
    "            try:\n",
    "                abstract = article['front']['article-meta']['abstract']\n",
    "                if type(abstract) == dict:\n",
    "                    try:\n",
    "                        resumo = abstract['p']['#text']\n",
    "                    except TypeError:\n",
    "                        try:\n",
    "                            if type(abstract['p']) == str:\n",
    "                                resumo = abstract['p']\n",
    "                            elif type(abstract['p']) == list:\n",
    "                                resumo = abstract['p'][0]['#text']\n",
    "                        except KeyError:\n",
    "                            try:\n",
    "                                resumo = abstract['sec'][0]['p']\n",
    "                            except:\n",
    "                                resumo = ''\n",
    "                        except:\n",
    "                            resumo = ''\n",
    "                    except:\n",
    "                        resumo = ''\n",
    "                elif type(abstract) == list:\n",
    "                    for ab in abstract:\n",
    "                        try:\n",
    "                            ab['@abstract-type']\n",
    "                        except:\n",
    "                            continue\n",
    "\n",
    "                        if ab['@abstract-type'] == 'author-highlights':\n",
    "\n",
    "                            if type(ab['p']) == dict:\n",
    "                                resumo = ab['p']['#text']\n",
    "                            elif type(ab['p']) == list:\n",
    "                                resumo = ''\n",
    "                                for p in ab['p']:\n",
    "                                    try:\n",
    "                                        p['#text']\n",
    "                                    except:\n",
    "                                        continue\n",
    "                                    resumo = \"%s\\n%s\" % (resumo, p['#text'])\n",
    "                else:\n",
    "                    resumo = ''\n",
    "            except Exception as e:\n",
    "                # Não tem resumo\n",
    "                resumo = ''\n",
    "                if debug:\n",
    "                    print(e.__class__.__name__, e, 'resumo')\n",
    "                    ipdb.set_trace()\n",
    "                    \n",
    "            ###\n",
    "            ### Fim da grosseria\n",
    "            ###\n",
    "            \n",
    "            if not pmc_id or not title:\n",
    "                # O mínimo é o LINK e o título para o documento ser incluso\n",
    "                continue\n",
    "            \n",
    "            documento = {}\n",
    "            documento['resumo'] = resumo\n",
    "            documento['html_url'] = \"%s%s\" % (self._article_url, pmc_id)\n",
    "            documento['autores'] = \",\".join(authors)\n",
    "            documento['doi'] = doi\n",
    "            documento['palavras_chaves'] = \",\".join(palavras_chave)\n",
    "            documento['titulo'] = title\n",
    "\n",
    "\n",
    "            append(documento)\n",
    "            \n",
    "        print('{:15s}{:6.3f}'.format(\"fetch\",time.time() - t_04))\n",
    "\n",
    "        return documentos\n",
    "\n",
    "\n",
    "class PubMed_Searcher(NCBI_Searcher):\n",
    "    \"\"\"Realiza pesquisas na base PubMed.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def _fields(self):\n",
    "        return ['Text Words']\n",
    "\n",
    "    @property\n",
    "    def _db(self):\n",
    "        return 'pubmed'\n",
    "\n",
    "    @property\n",
    "    def _sort_order(self):\n",
    "        return ''\n",
    "\n",
    "    @property\n",
    "    def _article_url(self):\n",
    "        return \"https://www.ncbi.nlm.nih.gov/pubmed/\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_data(p_art):\n",
    "        \"\"\"Vasculha o XML (um <PubmedArticle>) para encontrar a data de publicação\n",
    "        Se for encontrada uma data válida, retorna um datetime.\n",
    "        Se não, retorna uma string, que espera-se que contenha uma informação de data\"\"\"\n",
    "\n",
    "        if hasattr(p_art.PubDate.Year, \"text\"):\n",
    "            ano = p_art.PubDate.Year.text\n",
    "        elif hasattr(p_art.PubDate.MedlineDate, \"text\"):\n",
    "            ano = p_art.PubDate.MedlineDate.text[:8]\n",
    "\n",
    "        try:\n",
    "            data_pub_string = \"%s %s\" % (ano, NCBI_Searcher.deepgetter(p_art, 'PubDate.Month.text', default='Jan'))\n",
    "            data = datetime.strptime(data_pub_string, \"%Y %b\").date()\n",
    "        except:\n",
    "            try:\n",
    "                data_pub_string = \"%s %s\" % (ano, NCBI_Searcher.deepgetter(p_art, 'PubDate.Month.text', default='Jan'))\n",
    "                data = datetime.strptime(data_pub_string, \"%Y %m\").date()\n",
    "            except:\n",
    "                data = str(p_art.PubDate.text)\n",
    "\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_unique_id(p_art):\n",
    "        \"\"\"Vascula o XML (um <PubmedArticle>) para encontrar o ID único do artigo.\n",
    "        Se nao tiver DOI presente no XML, coloca o ID que tiver (esperado que seja o PubMed ID)\"\"\"\n",
    "\n",
    "        try:\n",
    "            unique_id = p_art.findAll(\"ArticleId\", {\"IdType\": \"doi\"})[0].text\n",
    "        except:\n",
    "            unique_id = p_art.findAll(\"ArticleId\")[0]\n",
    "            unique_id = \"%s%s\" % (unique_id['IdType'], unique_id.text)\n",
    "\n",
    "        return unique_id\n",
    "\n",
    "    def _get_article_metadata(self, *args):\n",
    "        id_list = ','.join([str(x) for x in args])\n",
    "\n",
    "        payload = {\"id\": id_list, \"db\": self._db, \"retmode\": \"xml\"}\n",
    "        payload.update(self.ncbi_register)\n",
    "        url = \"%s?%s\" % (self.fetch_url, urlencode(payload))\n",
    "\n",
    "        print(\"URL META: %s\" % url)\n",
    "\n",
    "        t_03 = time.time()\n",
    "        soup = bsoup(requests.get(url).content, \"xml\")\n",
    "        print('{:15s}{:6.3f}'.format(\"parse\",time.time() - t_03))\n",
    "\n",
    "        pubmed_articles = soup.findAll('PubmedArticle')\n",
    "\n",
    "        documentos = []\n",
    "        append = documentos.append\n",
    "\n",
    "        for p_art in pubmed_articles:\n",
    "            authors = [\"%s %s\" % (a.ForeName.text, a.LastName.text) for a in p_art.findAll(\"Author\")]\n",
    "            keywords = [k.text for k in p_art.findAll(\"Keyword\")]\n",
    "\n",
    "            documento = {}\n",
    "            documento['resumo'] = getattr(p_art.AbstractText, 'text', ' - ')\n",
    "            documento['html_url'] = \"%s%s\" % (self._article_url, p_art.PMID.text)\n",
    "            documento['autores'] = \",\".join(authors)\n",
    "            documento['doi'] = self._get_unique_id(p_art)\n",
    "            documento['palavras_chaves'] = \",\".join(keywords)\n",
    "            documento['titulo'] = p_art.ArticleTitle.text\n",
    "            data = self._get_data(p_art)\n",
    "            if type(data) == str:\n",
    "                documento['resumo'] = \"%s\\n%s\" % (data, documento['resumo'])\n",
    "            else:\n",
    "                documento['data'] = self._get_data(p_art)\n",
    "\n",
    "            append(documento)\n",
    "\n",
    "        return documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# termos de pesquisa relacionados a tecnologia\n",
    "technology_queryterms = [\n",
    "    'machine learning', 'deep learning', 'artificial intelligence', \n",
    "    'neural network', 'scoring system'\n",
    "]\n",
    "\n",
    "# termos de pesquisa relacionados a area da saude\n",
    "health_queryterms = [\n",
    "    'coronary artery disease', 'chest pain', 'heart disease', 'MACE', \n",
    "    'Acute Cardiac Complications'\n",
    "]\n",
    "\n",
    "queryterms = [technology_queryterms, health_queryterms]\n",
    "\n",
    "scimago_journals = [\"Journal of the American College of Cardiology\",\"Circulation\",\"European Heart Journal\"\n",
    "                    ,\"Circulation Research\",\"Nature Biotechnology\",\"Current Opinion in Biotechnology\"\n",
    "                   ,\"Annual Review of Biomedical Engineering\", \"Circulation: Cardiovascular Interventions\"]\n",
    "eigenfactor_journals = [\"Medical image Analysis\",\"Biomaterials\",\"Acta Biomaterialia\",\"Physics in medicine and biology\",\n",
    "                        \"IEEE TRANSACTIONS ON MEDICAL IMAGING\",\"COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE \"\n",
    "                       ,\"INTERNATIONAL JOURNAL OF CARDIOLOGY\", \"CARDIOVASCULAR RESEARCH\", \"HEART RHYTHM\"\n",
    "                       ,\"EUROPEAN JOURNAL OF CARDIO-THORACIC SURGERY\",\"JACC-Cardiovascular Interventions\"\n",
    "                        ,\"JOURNAL OF MOLECULAR AND CELLULAR CARDIOLOGY \", \"JACC-Cardiovascular Imaging \"\n",
    "                        ,\"Circulation-Heart Failure\", \"EUROPEAN JOURNAL OF HEART FAILURE\", \"EUROPACE\"\n",
    "                       ,\"CATHETERIZATION AND CARDIOVASCULAR INTERVENTIONS\", \"Journal of the American Heart Association\"\n",
    "                       ,\"JOURNAL OF THE AMERICAN SOCIETY OF ECHOCARDIOGRAPHY\", \"Circulation-Cardiovascular Imaging\"]\n",
    "\n",
    "journal = [\"BioMedical Engineering OnLine\",\n",
    "           \"Biomedical Engineering\"] + scimago_journals + eigenfactor_journals\n",
    "\n",
    "\n",
    "queryterms = [['machine learning'], ['ck mb']]\n",
    "# r = PMC_Searcher().search(queryterms=queryterms, max_records=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response        2.058\n",
      "Artigos encontrados:  223\n",
      "URL META: https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?id=5703994%2C5552265%2C5431941%2C5493079%2C5749823%2C4429500%2C4405523%2C4831781%2C4364461%2C4589532%2C3422834%2C2846872%2C5070523%2C4672312%2C4063215%2C5751801%2C3733825%2C4896250%2C3079734%2C5001212&db=pmc&retmode=xml&tool=Atena&email=ddddiegolima%40gmail.com\n",
      "response_M     14.294\n",
      "parse           3.058\n",
      "fetch          21.235\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Developing a Machine Learning ',\n",
       " 'Cardiac Function Improvement a',\n",
       " 'Precision Radiology: Predictin',\n",
       " '36th International Symposium o',\n",
       " 'Integrated genetic and epigene',\n",
       " 'Abstracts from the 37th Annual',\n",
       " 'Abstracts from the 38th Annual',\n",
       " 'Cardiac Troponin Is a Predicto',\n",
       " 'Abstracts for the 15th Interna',\n",
       " 'Programmable bio-nano-chip sys',\n",
       " 'Feature engineering combined w',\n",
       " 'A study of health effects of l',\n",
       " 'Learning statistical models of',\n",
       " 'Poster Session III\\nWednesday, ',\n",
       " 'Identifying Adverse Drug Event',\n",
       " 'Abstracts from the 9th Biennia',\n",
       " 'MUNDUS project: MUltimodal Neu',\n",
       " 'XXIV World Allergy Congress 20',\n",
       " 'Multiplexed Immunoassay Panel ',\n",
       " '25th Annual Computational Neur']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_01 = PMC_Searcher().search(queryterms=queryterms, max_records=20)\n",
    "[a['titulo'][:30] for a in r_01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response        0.843\n",
      "Artigos encontrados:  223\n",
      "URL META: https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?id=5703994%2C5552265%2C5431941%2C5493079%2C5749823%2C4429500%2C4405523%2C4831781%2C4364461%2C4589532%2C3422834%2C2846872%2C5070523%2C4672312%2C4063215%2C5751801%2C3733825%2C4896250%2C3079734%2C5001212%2C4669991%2C4261149%2C4672311%2C4943498%2C4672310%2C4389287%2C3654146%2C5592441%2C3204938%2C5374552%2C4869311%2C5042923%2C4871254%2C3639733%2C3751474%2C4212306%2C4261147%2C5149586%2C484364%2C5350766&db=pmc&retmode=xml&tool=Atena&email=ddddiegolima%40gmail.com\n",
      "response_M     46.773\n",
      "parse           9.446\n",
      "fetch          120.787\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Developing a Machine Learning ',\n",
       " 'Cardiac Function Improvement a',\n",
       " 'Precision Radiology: Predictin',\n",
       " '36th International Symposium o',\n",
       " 'Integrated genetic and epigene',\n",
       " 'Abstracts from the 37th Annual',\n",
       " 'Abstracts from the 38th Annual',\n",
       " 'Cardiac Troponin Is a Predicto',\n",
       " 'Abstracts for the 15th Interna',\n",
       " 'Programmable bio-nano-chip sys',\n",
       " 'Feature engineering combined w',\n",
       " 'A study of health effects of l',\n",
       " 'Learning statistical models of',\n",
       " 'Poster Session III\\nWednesday, ',\n",
       " 'Identifying Adverse Drug Event',\n",
       " 'Abstracts from the 9th Biennia',\n",
       " 'MUNDUS project: MUltimodal Neu',\n",
       " 'XXIV World Allergy Congress 20',\n",
       " 'Multiplexed Immunoassay Panel ',\n",
       " '25th Annual Computational Neur',\n",
       " 'UEG Week 2015 Poster Presentat',\n",
       " 'Poster Session III\\nWednesday, ',\n",
       " 'Poster Session II\\nTuesday, Dec',\n",
       " 'Proceedings of the 3rd IPLeiri',\n",
       " 'Poster Session I\\nMonday, Decem',\n",
       " 'High Field Small Animal Magnet',\n",
       " 'Abstracts from the 36th Annual',\n",
       " '26th Annual Computational Neur',\n",
       " 'Profiles and Majority Voting-B',\n",
       " '37th International Symposium o',\n",
       " 'Perfusion Pressure Cerebral In',\n",
       " 'ESICM LIVES 2016: part two',\n",
       " 'Choline metabolism-based molec',\n",
       " '2nd European Headache and Migr',\n",
       " 'Development and evaluation of ',\n",
       " 'UEG Week 2014 Poster Presentat',\n",
       " 'Poster Session I\\nMonday, Decem',\n",
       " 'ACNP 55th Annual Meeting: Post',\n",
       " 'POSTERS: On display in the Exh',\n",
       " 'ACTS Abstracts']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_02 = PMC_Searcher().search(queryterms=queryterms, max_records=40)\n",
    "[a['titulo'][:30] for a in r_02]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLIRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response        1.589\n",
      "Artigos encontrados:  223\n",
      "URL META: https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?id=5703994%2C5552265%2C5431941%2C5493079%2C5749823%2C4429500%2C4405523%2C4831781%2C4364461%2C4589532%2C3422834%2C2846872%2C5070523%2C4672312%2C4063215%2C5751801%2C3733825%2C4896250%2C3079734%2C5001212&db=pmc&retmode=xml&tool=Atena&email=ddddiegolima%40gmail.com\n",
      "response_M      9.921\n",
      "parse           1.040\n",
      "fetch           0.003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Developing a Machine Learning ',\n",
       " 'Cardiac Function Improvement a',\n",
       " 'Precision Radiology: Predictin',\n",
       " '36th International Symposium o',\n",
       " 'Integrated genetic and epigene',\n",
       " 'Abstracts from the 37th Annual',\n",
       " 'Abstracts from the 38th Annual',\n",
       " 'Cardiac Troponin Is a Predicto',\n",
       " 'Abstracts for the 15th Interna',\n",
       " 'Programmable bio-nano-chip sys',\n",
       " 'Feature engineering combined w',\n",
       " 'A study of health effects of l',\n",
       " 'Learning statistical models of',\n",
       " 'Poster Session III\\nWednesday, ',\n",
       " 'Identifying Adverse Drug Event',\n",
       " 'Abstracts from the 9th Biennia',\n",
       " 'MUNDUS project: MUltimodal Neu',\n",
       " 'XXIV World Allergy Congress 20',\n",
       " 'Multiplexed Immunoassay Panel ',\n",
       " '25th Annual Computational Neur']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_a = PMC_Searcher().search(queryterms=queryterms, max_records=20)\n",
    "[a['titulo'][:30] for a in r_a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response        1.016\n",
      "Artigos encontrados:  223\n",
      "URL META: https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?id=5703994%2C5552265%2C5431941%2C5493079%2C5749823%2C4429500%2C4405523%2C4831781%2C4364461%2C4589532%2C3422834%2C2846872%2C5070523%2C4672312%2C4063215%2C5751801%2C3733825%2C4896250%2C3079734%2C5001212%2C4669991%2C4261149%2C4672311%2C4943498%2C4672310%2C4389287%2C3654146%2C5592441%2C3204938%2C5374552%2C4869311%2C5042923%2C4871254%2C3639733%2C3751474%2C4212306%2C4261147%2C5149586%2C484364%2C5350766&db=pmc&retmode=xml&tool=Atena&email=ddddiegolima%40gmail.com\n",
      "response_M     18.025\n",
      "parse           3.223\n",
      "fetch           0.006\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Developing a Machine Learning ',\n",
       " 'Cardiac Function Improvement a',\n",
       " 'Precision Radiology: Predictin',\n",
       " '36th International Symposium o',\n",
       " 'Integrated genetic and epigene',\n",
       " 'Abstracts from the 37th Annual',\n",
       " 'Abstracts from the 38th Annual',\n",
       " 'Cardiac Troponin Is a Predicto',\n",
       " 'Abstracts for the 15th Interna',\n",
       " 'Programmable bio-nano-chip sys',\n",
       " 'Feature engineering combined w',\n",
       " 'A study of health effects of l',\n",
       " 'Learning statistical models of',\n",
       " 'Poster Session III\\nWednesday, ',\n",
       " 'Identifying Adverse Drug Event',\n",
       " 'Abstracts from the 9th Biennia',\n",
       " 'MUNDUS project: MUltimodal Neu',\n",
       " 'XXIV World Allergy Congress 20',\n",
       " 'Multiplexed Immunoassay Panel ',\n",
       " '25th Annual Computational Neur']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_b = PMC_Searcher().search(queryterms=queryterms, max_records=40)\n",
    "[a['titulo'][:30] for a in r_b[:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response        1.841\n",
      "Artigos encontrados:  223\n",
      "URL META: https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?id=5703994%2C5552265%2C5431941%2C5493079%2C5749823%2C4429500%2C4405523%2C4831781%2C4364461%2C4589532%2C3422834%2C2846872%2C5070523%2C4672312%2C4063215%2C5751801%2C3733825%2C4896250%2C3079734%2C5001212%2C4669991%2C4261149%2C4672311%2C4943498%2C4672310%2C4389287%2C3654146%2C5592441%2C3204938%2C5374552%2C4869311%2C5042923%2C4871254%2C3639733%2C3751474%2C4212306%2C4261147%2C5149586%2C484364%2C5350766%2C4682919%2C4272368%2C5592442%2C1683569%2C5575665%2C4870725%2C4244175%2C4061745%2C3044641%2C5009929%2C3060650%2C4444413%2C4944947%2C5042925%2C5615764%2C5010413%2C4189906%2C4284756%2C1913720%2C5310653&db=pmc&retmode=xml&tool=Atena&email=ddddiegolima%40gmail.com\n",
      "response_M     55.852\n",
      "parse           4.245\n",
      "fetch           0.006\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Developing a Machine Learning ',\n",
       " 'Cardiac Function Improvement a',\n",
       " 'Precision Radiology: Predictin',\n",
       " '36th International Symposium o',\n",
       " 'Integrated genetic and epigene',\n",
       " 'Abstracts from the 37th Annual',\n",
       " 'Abstracts from the 38th Annual',\n",
       " 'Cardiac Troponin Is a Predicto',\n",
       " 'Abstracts for the 15th Interna',\n",
       " 'Programmable bio-nano-chip sys',\n",
       " 'Feature engineering combined w',\n",
       " 'A study of health effects of l',\n",
       " 'Learning statistical models of',\n",
       " 'Poster Session III\\nWednesday, ',\n",
       " 'Identifying Adverse Drug Event',\n",
       " 'Abstracts from the 9th Biennia',\n",
       " 'MUNDUS project: MUltimodal Neu',\n",
       " 'XXIV World Allergy Congress 20',\n",
       " 'Multiplexed Immunoassay Panel ',\n",
       " '25th Annual Computational Neur']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_c = PMC_Searcher().search(queryterms=queryterms, max_records=60)\n",
    "[a['titulo'][:30] for a in r_b[:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response        1.722\n",
      "Artigos encontrados:  223\n",
      "sleeping\n",
      "response        0.655\n",
      "URL META: https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?id=4669991%2C4261149%2C4672311%2C4943498%2C4672310%2C4389287%2C3654146%2C5592441%2C3204938%2C5374552%2C4869311%2C5042923%2C4871254%2C3639733%2C3751474%2C4212306%2C4261147%2C5149586%2C484364%2C5350766&db=pmc&retmode=xml&tool=Atena&email=ddddiegolima%40gmail.com\n",
      "response        0.669\n",
      "URL META: https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?id=1501050%2C5461533%2C4701479%2C5650402%2C4070603%2C3324437%2C2683819%2C3204897%2C4070608%2C4244172%2C3533624%2C2872253%2C3533621%2C5687535%2C5766549%2C5637785%2C3481065%2C5330968%2C5029834%2C2219573&db=pmc&retmode=xml&tool=Atena&email=ddddiegolima%40gmail.com\n",
      "response        0.690\n",
      "URL META: https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?id=4682919%2C4272368%2C5592442%2C1683569%2C5575665%2C4870725%2C4244175%2C4061745%2C3044641%2C5009929%2C3060650%2C4444413%2C4944947%2C5042925%2C5615764%2C5010413%2C4189906%2C4284756%2C1913720%2C5310653&db=pmc&retmode=xml&tool=Atena&email=ddddiegolima%40gmail.com\n",
      "sleeping\n",
      "response        0.952\n",
      "URL META: https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?id=4371020%2C2492837%2C2742725%2C4858066%2C5408529%2C1808491%2C4192695%2C3163006%2C5139455%2C3615018%2C5103253%2C3179438%2C2798987%2C5647556%2C5374592%2C5596409%2C3589539%2C3527209%2C4300392%2C5095516&db=pmc&retmode=xml&tool=Atena&email=ddddiegolima%40gmail.com\n",
      "response        1.503\n",
      "URL META: https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?id=4954614%2C5042924%2C1871910%2C4922900%2C3848855%2C4785292%2C5231296%2C4640374%2C5225574%2C5350815%2C5833337%2C3618247%2C1913172%2C4094927%2C5749541%2C4643087%2C164657%2C3534347%2C4912513%2C2665694&db=pmc&retmode=xml&tool=Atena&email=ddddiegolima%40gmail.com\n",
      "response        1.568\n",
      "URL META: https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?id=2809242%2C153066%2C4654731%2C4042686%2C5662084%2C4014501%2C5439753%2C1871956%2C4577949%2C5825102%2C2547355%2C5539728%2C5605184%2C4866447%2C5461530%2C3337957%2C3098094%2C5590667%2C2877558%2C2447491&db=pmc&retmode=xml&tool=Atena&email=ddddiegolima%40gmail.com\n",
      "sleeping\n",
      "response        0.689\n",
      "URL META: https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?id=4887520%2C3617391%2C1371213%2C1760628%2C1760606%2C1760633%2C1760625%2C1760617%2C1760601%2C1760597%2C1760593%2C1760587%2C1339983%2C5180407%2C5810281%2C4782522%2C4210203%2C2846957%2C3576491%2C4855472&db=pmc&retmode=xml&tool=Atena&email=ddddiegolima%40gmail.com\n",
      "response        0.748\n",
      "URL META: https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?id=2586628%2C4980779%2C5161449%2C5590963%2C3371856%2C4587627%2C5858585%2C5759841%2C5834122%2C1479840%2C1524991%2C5528629%2C5675491%2C5123388%2C5123387%2C4029371%2C5341997%2C4342276%2C4726047%2C5845264&db=pmc&retmode=xml&tool=Atena&email=ddddiegolima%40gmail.com\n",
      "response        0.764\n",
      "URL META: https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?id=4410667%2C4460761%2C4506491%2C5069785%2C5797586%2C3223728%2C3274206%2C1413562%2C2865495%2C2447453%2C3468564%2C1780111%2C2448380%2C2447404%2C2447381%2C2448432%2C2447231%2C4672482%2C5225387%2C5368254&db=pmc&retmode=xml&tool=Atena&email=ddddiegolima%40gmail.com\n",
      "response_M      1.604\n",
      "parse           0.630\n",
      "fetch           0.001\n",
      "response_M      2.871\n",
      "parse           0.704\n",
      "fetch           0.001\n",
      "response        0.768\n",
      "URL META: https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?id=5452349%2C4118789%2C5578953%2C5676736%2C4757180%2C5550551%2C1915515%2C2815462%2C1765785%2C2557557%2C2020767%2C225489%2C1941138%2C1090701%2C2335115%2C2279858%2C1230847%2C1460871%2C462743%2C1241897&db=pmc&retmode=xml&tool=Atena&email=ddddiegolima%40gmail.com\n",
      "response        0.786\n",
      "URL META: https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?id=1242079%2C4928264%2C3541939&db=pmc&retmode=xml&tool=Atena&email=ddddiegolima%40gmail.com\n",
      "response_M      2.120\n",
      "parse           0.543\n",
      "fetch           0.000\n",
      "response_M      2.672\n",
      "parse           0.498\n",
      "fetch           0.001\n",
      "response_M      1.132\n",
      "parse           0.014\n",
      "fetch           0.000\n",
      "response_M      4.697\n",
      "response_M      2.176\n",
      "parse           0.580\n",
      "fetch           0.000\n",
      "parse           1.602\n",
      "fetch           0.002\n",
      "response_M      9.159\n",
      "parse           1.131\n",
      "fetch           0.003\n",
      "response_M      6.217\n",
      "response_M     10.646\n",
      "parse           1.865\n",
      "fetch           0.001\n",
      "parse           2.399\n",
      "fetch           0.001\n",
      "response_M     19.341\n",
      "parse           2.574\n",
      "fetch           0.002\n",
      "thread fetching  20\n",
      "thread fetching  19\n",
      "thread fetching  20\n",
      "thread fetching  20\n",
      "thread fetching  20\n",
      "thread fetching  20\n",
      "thread fetching  20\n",
      "thread fetching  20\n",
      "thread fetching  20\n",
      "thread fetching  20\n",
      "thread fetching  3\n",
      "URL META: https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?id=5703994%2C5552265%2C5431941%2C5493079%2C5749823%2C4429500%2C4405523%2C4831781%2C4364461%2C4589532%2C3422834%2C2846872%2C5070523%2C4672312%2C4063215%2C5751801%2C3733825%2C4896250%2C3079734%2C5001212&db=pmc&retmode=xml&tool=Atena&email=ddddiegolima%40gmail.com\n",
      "response_M     14.379\n",
      "parse           1.249\n",
      "fetch           0.003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['UEG Week 2015 Poster Presentat',\n",
       " 'Poster Session III\\nWednesday, ',\n",
       " 'Poster Session II\\nTuesday, Dec',\n",
       " 'Proceedings of the 3rd IPLeiri',\n",
       " 'Poster Session I\\nMonday, Decem',\n",
       " 'High Field Small Animal Magnet',\n",
       " 'Abstracts from the 36th Annual',\n",
       " '26th Annual Computational Neur',\n",
       " 'Profiles and Majority Voting-B',\n",
       " '37th International Symposium o',\n",
       " 'Perfusion Pressure Cerebral In',\n",
       " 'ESICM LIVES 2016: part two',\n",
       " 'Choline metabolism-based molec',\n",
       " '2nd European Headache and Migr',\n",
       " 'Development and evaluation of ',\n",
       " 'UEG Week 2014 Poster Presentat',\n",
       " 'Poster Session I\\nMonday, Decem',\n",
       " 'ACNP 55th Annual Meeting: Post',\n",
       " 'On display in the Exhibition H',\n",
       " 'ACTS Abstracts']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_d = PMC_Searcher().search(queryterms=queryterms)\n",
    "[a['titulo'][:30] for a in r_d[:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "counts = Counter([a['titulo'] for a in r_d])\n",
    "{k:v for k,v in counts.items() if v > 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([a['titulo'] for a in r_d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
