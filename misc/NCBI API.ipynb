{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod\n",
    "from urllib.parse import urlencode\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup as bsoup\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCBI_Searcher(metaclass=ABCMeta):\n",
    "    \"\"\" 'Interface' que define a utilização da API das databases da NCBI.\n",
    "    \"\"\"\n",
    "\n",
    "    search_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi'\n",
    "    meta_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi'\n",
    "    fetch_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi'\n",
    "\n",
    "    def search(self, queryterms: list = None, search_type: str = None,\n",
    "               start_year: int = 1900, end_year: int = None,\n",
    "               max_records: int = 20, start_record: int = 0,\n",
    "               author: str = None, journal: str = None):\n",
    "        \"\"\"\n",
    "        Realiza uma pesquisa NCBI.\n",
    "        @param queryterms: list of lists. Terms within the same list are\n",
    "            separated by an OR. Lists are separated by an AND\n",
    "        @param search_type: meta_data or querytext.\n",
    "            meta_data: This field enables a free-text search of all\n",
    "                configured metadata fields and the abstract.\n",
    "            querytext: This field enables a free-text search of all\n",
    "                fields.\n",
    "        @param start_year: Start value of Publication Year to restrict results by.\n",
    "        @param end_year: End value of Publication Year to restrict results by.\n",
    "        @param max_records: The number of records to fetch.\n",
    "        @param start_record: Sequence number of first record to fetch.\n",
    "        @param author: An author's name. Searches both first name and last name\n",
    "            Accepts a list of author names too.\n",
    "        @param journal: An author's name. Accepts a list of journals too.\n",
    "        @return: a dictionaries list whose keys are compatible with Documento model.\n",
    "        \"\"\"\n",
    "\n",
    "        term = self._search_term(queryterms, search_type=search_type)\n",
    "        if author:\n",
    "            author = [author] if type(author) == str else author\n",
    "            author = ['%s[Author]' % a for a in author]\n",
    "            term = \"%s AND (%s)\" % (term, \" OR \".join(author) )\n",
    "\n",
    "        if journal:\n",
    "            journal = [journal] if type(journal) == str else journal\n",
    "            journal = ['\"%s\"[Journal]' % j for j in journal]\n",
    "            term = \"%s AND (%s)\" % (term, \" OR \".join(journal) )\n",
    "\n",
    "        fixed_payload = {\"retmode\": \"json\", \"datetype\": \"pdat\",\n",
    "                         \"db\": self._db, \"sort\": self._sort_order}\n",
    "        payload = {\"term\": term,\n",
    "                   \"retmax\": max_records, \"retstart\": start_record,\n",
    "                   \"mindate\": start_year or '', \"maxdate\": end_year or datetime.now().year}\n",
    "        payload.update(fixed_payload)\n",
    "\n",
    "        url = \"%s?%s\" % (self.search_url, urlencode(payload))\n",
    "        \n",
    "        print(\"URL SEARCH: %s\" % url)\n",
    "\n",
    "        print(\"Você pode realizar essa mesma busca no navegador com o termo de busca:\\n%s\" % term)\n",
    "\n",
    "        response = requests.get(url).json()['esearchresult']\n",
    "\n",
    "        print('QTD. resultados: %s' % response['count'])\n",
    "\n",
    "        id_list = response['idlist']\n",
    "\n",
    "        if id_list:\n",
    "            return self._get_article_metadata(*id_list)\n",
    "        return []\n",
    "\n",
    "    def _search_term(self, queryterms: list, search_type: str = None):\n",
    "        \"\"\"Monta o termo de pesquisa completo para mandar para a API.\"\"\"\n",
    "\n",
    "        if search_type in ['querytext', None]:\n",
    "            # Retorna simplesmente a busca concatenando com os OR's e AND's\n",
    "            return \"(%s)\" % \" AND \".join([\"(%s)\" % \" OR \".join(orses) for orses in queryterms])\n",
    "        elif search_type != 'meta_data':\n",
    "            raise Exception('Tipo de pesquisa não faz sentido: %s\\nTipos suportados:' % search_type)\n",
    "\n",
    "        # Retorna concacentando com os OR'S e AND's, mas embutindo também os campos de pesquisa em cada termo\n",
    "        queryterms = [[self._embutir_fields(orses) for orses in andes] for andes in queryterms]\n",
    "        return \"(%s)\" % \" AND \".join([\"(%s)\" % \" OR \".join(orses) for orses in queryterms])\n",
    "\n",
    "    def _embutir_fields(self, term: str):\n",
    "        \"\"\"Faz uma transformação, embutindo fields no termo de pesquisa.\n",
    "        Isso é para poder realizar a pesquisa em apenas alguns campos ao invés de todos.\n",
    "        Exemplo: sendo self.__fields = ['title', 'abstract'],\n",
    "        a chamada\n",
    "        `self._embutir_fields(\"machine learning\")`\n",
    "        Transforma:\n",
    "            machine learning ---> (machine learning[title] OR machine learning[abstract])\n",
    "        \"\"\"\n",
    "\n",
    "        return \"(%s)\" % \" OR \".join([\"%s[%s]\" % (term, field) for field in self._fields])\n",
    "\n",
    "    @staticmethod\n",
    "    def deepgetter(obj, attrs, default=None):\n",
    "        \"\"\"Faz uma chamada sucessiva da função getattr, para ir pegando os atributos\n",
    "        de um objeto.\n",
    "        Exemplo:\n",
    "        deepgetter(Cidade, 'regiao.pais') é equivalente a fazer Cidade.regiao.pais\n",
    "        \"\"\"\n",
    "        getter = lambda x, y: getattr(x, y, default)\n",
    "        return reduce(getter, attrs.split('.'), obj)\n",
    "\n",
    "    @abstractmethod\n",
    "    def _get_article_metadata(self, *args):\n",
    "        \"\"\"Cada subclasse deverá implementar a função que pega o retorno da API e transforma numa lista de dicionários\n",
    "        no formato do modelo Documento.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def _fields(self):\n",
    "        \"\"\"Cada subclasse deverá definir quais serão os campos de pesquisa de cada termo.\n",
    "        O retorno deverá ser uma lista de fields.\n",
    "        Exemplo:\n",
    "        return ['title', 'abstract']\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def _db(self):\n",
    "        \"\"\"Cada subclasse deverá definir o seu banco.\n",
    "        Exemplo:\n",
    "        return 'pmc'\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def _sort_order(self):\n",
    "        \"\"\"Cada classe deverá definir o parâmetro sort_order.\n",
    "        Exemplo:\n",
    "        return 'Journal'\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def _article_url(self):\n",
    "        \"\"\"Cada classe deverá definir a URL da página de um artigo.\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class PMC_Searcher(NCBI_Searcher):\n",
    "    \"\"\"Realiza pesquisas na base PMC.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def _fields(self):\n",
    "        return ['Abstract', 'Body - Key Terms', 'MeSH Terms',\n",
    "                'MeSH Major Topic', 'Methods - Key Terms']\n",
    "\n",
    "    @property\n",
    "    def _db(self):\n",
    "        return 'pmc'\n",
    "\n",
    "    @property\n",
    "    def _sort_order(self):\n",
    "        return 'relevance'\n",
    "\n",
    "    @property\n",
    "    def _article_url(self):\n",
    "        return 'https://www.ncbi.nlm.nih.gov/pmc/articles/'\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_unique_id(p_art):\n",
    "        \"\"\"Vascula o XML (um <PubmedArticle>) para encontrar o ID único do artigo.\n",
    "        Se nao tiver DOI presente no XML, coloca o ID que tiver (esperado que seja o PubMed ID)\"\"\"\n",
    "        \n",
    "        try:\n",
    "            unique_id = p_art.findAll(\"article-id\", {\"pub-id-type\": \"doi\"})[0].text\n",
    "        except:\n",
    "            unique_id = p_art.findAll(\"article-id\")[0]\n",
    "            unique_id = \"%s%s\" % (unique_id['pub-id-type'], unique_id.text)\n",
    "            \n",
    "        return unique_id\n",
    "\n",
    "    def _get_article_metadata(self, *args):\n",
    "        id_list = ','.join([str(x) for x in args])\n",
    "\n",
    "        payload = {\"id\": id_list, \"db\": self._db, \"retmode\": \"xml\"}\n",
    "        url = \"%s?%s\" % (self.fetch_url, urlencode(payload))\n",
    "        print(\"URL META: %s\" % url)\n",
    "\n",
    "        soup = bsoup(requests.get(url).content, \"xml\")\n",
    "\n",
    "        pmc_articles = soup.findAll('article')\n",
    "\n",
    "        documentos = []\n",
    "        append = documentos.append\n",
    "\n",
    "        for p_art in pmc_articles:\n",
    "            author_list = p_art.findAll(\"contrib\", {\"contrib-type\": \"author\"})\n",
    "            authors = []\n",
    "            for author in author_list:\n",
    "                try:\n",
    "                    authors.append(\"%s %s\" % (getattr(author, \"given-names\").text, author.surname.text))\n",
    "                except:\n",
    "                    authors.append(author.text)\n",
    "\n",
    "            keywords = [k.text for k in p_art.findAll(\"kwd\")]\n",
    "            pmc_id = soup.findAll(\"article-id\", {\"pub-id-type\": 'pmc'})[0].text\n",
    "\n",
    "            documento = {}\n",
    "            documento['resumo'] = getattr(p_art.abstract, 'text', ' - ')\n",
    "            documento['html_url'] = \"%s%s\" % (self._article_url, pmc_id)\n",
    "            documento['autores'] = \",\".join(authors)\n",
    "            documento['doi'] = self._get_unique_id(p_art)\n",
    "            documento['palavras_chaves'] = \",\".join(keywords)\n",
    "            documento['titulo'] = getattr(p_art, \"article-title\").text\n",
    "\n",
    "            try:\n",
    "                pub_date = p_art.findAll(\"pub-date\", {\"pub-type\": \"epub\"})[0]\n",
    "            except:\n",
    "                pub_date = p_art.findAll(\"pub-date\", {\"pub-type\": \"ppub\"})[0]\n",
    "\n",
    "            data_pub_string = \"%s %s\" % (pub_date.year.text, pub_date.month.text)\n",
    "            \n",
    "            try:\n",
    "                documento['data'] = datetime.strptime(data_pub_string, \"%Y %m\").date()\n",
    "            except:\n",
    "                documento['data'] = datetime.strptime(data_pub_string, \"%Y %b\").date()\n",
    "\n",
    "            append(documento)\n",
    "\n",
    "        return documentos\n",
    "\n",
    "\n",
    "class PubMed_Searcher(NCBI_Searcher):\n",
    "    \"\"\"Realiza pesquisas na base PubMed.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def _fields(self):\n",
    "        return ['Text Words']\n",
    "\n",
    "    @property\n",
    "    def _db(self):\n",
    "        return 'pubmed'\n",
    "\n",
    "    @property\n",
    "    def _sort_order(self):\n",
    "        return ''\n",
    "\n",
    "    @property\n",
    "    def _article_url(self):\n",
    "        return \"https://www.ncbi.nlm.nih.gov/pubmed/\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_data(p_art):\n",
    "        \"\"\"Vasculha o XML (um <PubmedArticle>) para encontrar a data de publicação\n",
    "        Se for encontrada uma data válida, retorna um datetime.\n",
    "        Se não, retorna uma string, que espera-se que contenha uma informação de data\"\"\"\n",
    "        \n",
    "        if hasattr(p_art.PubDate.Year, \"text\"):\n",
    "            ano = p_art.PubDate.Year.text\n",
    "        elif hasattr(p_art.PubDate.MedlineDate, \"text\"):\n",
    "            ano = p_art.PubDate.MedlineDate.text[:8]\n",
    "        \n",
    "        try:\n",
    "            data_pub_string = \"%s %s\" % (ano, self.deepgetter(p_art, 'PubDate.Month.text', default='Jan'))\n",
    "            data = datetime.strptime(data_pub_string, \"%Y %b\").date()\n",
    "        except:\n",
    "            try:\n",
    "                data_pub_string = \"%s %s\" % (ano, self.deepgetter(p_art, 'PubDate.Month.text', default='Jan'))\n",
    "                data = datetime.strptime(data_pub_string, \"%Y %m\").date()\n",
    "            except:\n",
    "                data = str(p_art.PubDate.text)\n",
    "\n",
    "        return data\n",
    "        \n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def _get_unique_id(p_art):\n",
    "        \"\"\"Vascula o XML (um <PubmedArticle>) para encontrar o ID único do artigo.\n",
    "        Se nao tiver DOI presente no XML, coloca o ID que tiver (esperado que seja o PubMed ID)\"\"\"\n",
    "        \n",
    "        try:\n",
    "            unique_id = p_art.findAll(\"ArticleId\", {\"IdType\": \"doi\"})[0].text\n",
    "        except:\n",
    "            unique_id = p_art.findAll(\"ArticleId\")[0]\n",
    "            unique_id = \"%s%s\" % (unique_id['IdType'], unique_id.text)\n",
    "            \n",
    "        return unique_id\n",
    "\n",
    "    def _get_article_metadata(self, *args):\n",
    "        id_list = ','.join([str(x) for x in args])\n",
    "\n",
    "        payload = {\"id\": id_list, \"db\": self._db, \"retmode\": \"xml\"}\n",
    "        url = \"%s?%s\" % (self.fetch_url, urlencode(payload))\n",
    "        \n",
    "        print(\"URL META: %s\" % url)\n",
    "\n",
    "        soup = bsoup(requests.get(url).content, \"xml\")\n",
    "\n",
    "        pubmed_articles = soup.findAll('PubmedArticle')\n",
    "\n",
    "        documentos = []\n",
    "        append = documentos.append\n",
    "\n",
    "        for p_art in pubmed_articles:\n",
    "            authors = [\"%s %s\" % (a.ForeName.text, a.LastName.text) for a in p_art.findAll(\"Author\")]\n",
    "            keywords = [k.text for k in p_art.findAll(\"Keyword\")]\n",
    "            \n",
    "            documento = {}\n",
    "            documento['resumo'] = getattr(p_art.AbstractText, 'text', ' - ')\n",
    "            documento['html_url'] = \"%s%s\" % (self._article_url, p_art.PMID.text)\n",
    "            documento['autores'] = \",\".join(authors)\n",
    "            documento['doi'] = self._get_unique_id(p_art)\n",
    "            documento['palavras_chaves'] = \",\".join(keywords)\n",
    "            documento['titulo'] = p_art.ArticleTitle.text\n",
    "            data = self._get_data(p_art)\n",
    "            if type(data) == str:\n",
    "                documento['resumo'] = \"%s\\n%s\" % (data, documento['resumo'])\n",
    "            else:\n",
    "                documento['data'] = self._get_data(p_art)\n",
    "\n",
    "            append(documento)\n",
    "\n",
    "        return documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# termos de pesquisa relacionados a tecnologia\n",
    "technology_queryterms = [\n",
    "    'machine learning', 'deep learning', 'artificial intelligence', \n",
    "    'neural network', 'scoring system'\n",
    "]\n",
    "\n",
    "# termos de pesquisa relacionados a area da saude\n",
    "health_queryterms = [\n",
    "    'coronary artery disease', 'chest pain', 'heart disease', 'MACE', \n",
    "    'Acute Cardiac Complications'\n",
    "]\n",
    "\n",
    "queryterms = [technology_queryterms, health_queryterms]\n",
    "\n",
    "# lista de revistas que delimitam a busca\n",
    "journal = [\"BioMedical Engineering OnLine\",\n",
    "           \"Biomedical Engineering\"]\n",
    "\n",
    "# r = PMC_Searcher().search(queryterms=queryterms, start_year=2008, max_records=5, journal=journal)\n",
    "# [a['titulo'][:50] for a in r]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryterms = [['artificial intelligence'], ['cardiovascular death']]\n",
    "\n",
    "r = PubMed_Searcher().search(queryterms=queryterms, max_records=5)\n",
    "[a['titulo'][:50] for a in r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL SEARCH: https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?term=%28%28rup%29+AND+%28cardiovascular+disease%29%29&retmax=5&retstart=0&mindate=1900&maxdate=2018&retmode=json&datetype=pdat&db=pmc&sort=relevance\n",
      "Você pode realizar essa mesma busca no navegador com o termo de busca:\n",
      "((rup) AND (cardiovascular disease))\n",
      "QTD. resultados: 110\n",
      "URL META: https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?id=5095330%2C5388187%2C1888177%2C4212304%2C3264995&db=pmc&retmode=xml\n",
      "> \u001b[0;32m<ipython-input-37-64b6fb5ce4d9>\u001b[0m(206)\u001b[0;36m_get_article_metadata\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    205 \u001b[0;31m            \u001b[0;32mimport\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 206 \u001b[0;31m            \u001b[0mdocumento\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'doi'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_unique_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_art\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    207 \u001b[0;31m            \u001b[0mdocumento\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'palavras_chaves'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\",\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> continue\n",
      "> \u001b[0;32m<ipython-input-37-64b6fb5ce4d9>\u001b[0m(205)\u001b[0;36m_get_article_metadata\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    204 \u001b[0;31m            \u001b[0mdocumento\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'autores'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\",\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauthors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 205 \u001b[0;31m            \u001b[0;32mimport\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    206 \u001b[0;31m            \u001b[0mdocumento\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'doi'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_unique_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_art\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> continue\n",
      "> \u001b[0;32m<ipython-input-37-64b6fb5ce4d9>\u001b[0m(206)\u001b[0;36m_get_article_metadata\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    205 \u001b[0;31m            \u001b[0;32mimport\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 206 \u001b[0;31m            \u001b[0mdocumento\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'doi'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_unique_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_art\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    207 \u001b[0;31m            \u001b[0mdocumento\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'palavras_chaves'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\",\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> continue\n",
      "> \u001b[0;32m<ipython-input-37-64b6fb5ce4d9>\u001b[0m(205)\u001b[0;36m_get_article_metadata\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    204 \u001b[0;31m            \u001b[0mdocumento\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'autores'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\",\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauthors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 205 \u001b[0;31m            \u001b[0;32mimport\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    206 \u001b[0;31m            \u001b[0mdocumento\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'doi'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_unique_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_art\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> continue\n",
      "> \u001b[0;32m<ipython-input-37-64b6fb5ce4d9>\u001b[0m(206)\u001b[0;36m_get_article_metadata\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    205 \u001b[0;31m            \u001b[0;32mimport\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 206 \u001b[0;31m            \u001b[0mdocumento\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'doi'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_unique_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_art\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    207 \u001b[0;31m            \u001b[0mdocumento\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'palavras_chaves'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\",\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> continue\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Tongxinluo modulates cytokine secretion by cardiac',\n",
       " 'Increased Vascular Permeability Measured With an A',\n",
       " 'Myocardial diseases of animals.',\n",
       " 'UEG Week 2014 Oral Presentations',\n",
       " 'Circulating levels of vascular endothelial markers']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queryterms = [['rup'], ['cardiovascular disease']]\n",
    "\n",
    "r = PMC_Searcher().search(queryterms=queryterms, max_records=5)\n",
    "[a['titulo'][:50] for a in r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
