{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod\n",
    "from urllib.parse import urlencode\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup as bsoup\n",
    "\n",
    "class NCBI_Searcher(metaclass=ABCMeta):\n",
    "    \"\"\" 'Interface' que define a utilização da API das databases da NCBI.\n",
    "    \"\"\"\n",
    "\n",
    "    search_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi'\n",
    "    meta_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi'\n",
    "    fetch_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi'\n",
    "\n",
    "    def search(self, queryterms: list = None, search_type: str = None,\n",
    "               start_year: int = 1900, end_year: int = None,\n",
    "               max_records: int = 20, start_record: int = 0,\n",
    "               author: str = None, journal: str = None):\n",
    "        \"\"\"\n",
    "        Realiza uma pesquisa NCBI.\n",
    "        @param queryterms: list of lists. Terms within the same list are\n",
    "            separated by an OR. Lists are separated by an AND\n",
    "        @param search_type: meta_data or querytext.\n",
    "            meta_data: This field enables a free-text search of all\n",
    "                configured metadata fields and the abstract.\n",
    "            querytext: This field enables a free-text search of all\n",
    "                fields.\n",
    "        @param start_year: Start value of Publication Year to restrict results by.\n",
    "        @param end_year: End value of Publication Year to restrict results by.\n",
    "        @param max_records: The number of records to fetch.\n",
    "        @param start_record: Sequence number of first record to fetch.\n",
    "        @param author: An author's name. Searches both first name and last name\n",
    "            Accepts a list of author names too.\n",
    "        @param journal: An author's name. Accepts a list of journals too.\n",
    "        @return: a dictionaries list whose keys are compatible with Documento model.\n",
    "        \"\"\"\n",
    "\n",
    "        term = self._search_term(queryterms, search_type=search_type)\n",
    "        if author:\n",
    "            author = [author] if type(author) == str else author\n",
    "            author = ['%s[Author]' % a for a in author]\n",
    "            term = \"%s AND (%s)\" % (term, \" OR \".join(author) )\n",
    "\n",
    "        if journal:\n",
    "            journal = [journal] if type(journal) == str else journal\n",
    "            journal = ['\"%s\"[Journal]' % j for j in journal]\n",
    "            term = \"%s AND (%s)\" % (term, \" OR \".join(journal) )\n",
    "\n",
    "        fixed_payload = {\"retmode\": \"json\", \"datetype\": \"pdat\",\n",
    "                         \"db\": self._db, \"sort\": self._sort_order}\n",
    "        payload = {\"term\": term,\n",
    "                   \"retmax\": max_records, \"retstart\": start_record,\n",
    "                   \"mindate\": start_year, \"maxdate\": end_year or datetime.now().year}\n",
    "        payload.update(fixed_payload)\n",
    "\n",
    "        url = \"%s?%s\" % (self.search_url, urlencode(payload))\n",
    "\n",
    "        print(\"Você pode realizar essa mesma busca no navegador com o termo de busca:\\n%s\" % term)\n",
    "\n",
    "        response = requests.get(url).json()['esearchresult']\n",
    "\n",
    "        print('QTD. resultados: %s' % response['count'])\n",
    "\n",
    "        id_list = response['idlist']\n",
    "\n",
    "        if id_list:\n",
    "            return self._get_article_metadata(*id_list)\n",
    "        return []\n",
    "\n",
    "    def _search_term(self, queryterms: list, search_type: str = None):\n",
    "        \"\"\"Monta o termo de pesquisa completo para mandar para a API.\"\"\"\n",
    "\n",
    "        if search_type in ['querytext', None]:\n",
    "            # Retorna simplesmente a busca concatenando com os OR's e AND's\n",
    "            return \"(%s)\" % \" AND \".join([\"(%s)\" % \" OR \".join(orses) for orses in queryterms])\n",
    "        elif search_type != 'meta_data':\n",
    "            raise Exception('Tipo de pesquisa não faz sentido: %s\\nTipos suportados:' % search_type)\n",
    "\n",
    "        # Retorna concacentando com os OR'S e AND's, mas embutindo também os campos de pesquisa em cada termo\n",
    "        queryterms = [[self._embutir_fields(orses) for orses in andes] for andes in queryterms]\n",
    "        return \"(%s)\" % \" AND \".join([\"(%s)\" % \" OR \".join(orses) for orses in queryterms])\n",
    "\n",
    "    def _embutir_fields(self, term: str):\n",
    "        \"\"\"Faz uma transformação, embutindo fields no termo de pesquisa.\n",
    "        Isso é para poder realizar a pesquisa em apenas alguns campos ao invés de todos.\n",
    "        Exemplo: sendo self.__fields = ['title', 'abstract'],\n",
    "        a chamada\n",
    "        `self._embutir_fields(\"machine learning\")`\n",
    "        Transforma:\n",
    "            machine learning ---> (machine learning[title] OR machine learning[abstract])\n",
    "        \"\"\"\n",
    "\n",
    "        return \"(%s)\" % \" OR \".join([\"%s[%s]\" % (term, field) for field in self._fields])\n",
    "\n",
    "    @staticmethod\n",
    "    def deepgetter(obj, attrs, default=None):\n",
    "        \"\"\"Faz uma chamada sucessiva da função getattr, para ir pegando os atributos\n",
    "        de um objeto.\n",
    "        Exemplo:\n",
    "        deepgetter(Cidade, 'regiao.pais') é equivalente a fazer Cidade.regiao.pais\n",
    "        \"\"\"\n",
    "        getter = lambda x, y: getattr(x, y, default)\n",
    "        return reduce(getter, attrs.split('.'), obj)\n",
    "\n",
    "    @abstractmethod\n",
    "    def _get_article_metadata(self, *args):\n",
    "        \"\"\"Cada subclasse deverá implementar a função que pega o retorno da API e transforma numa lista de dicionários\n",
    "        no formato do modelo Documento.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def _fields(self):\n",
    "        \"\"\"Cada subclasse deverá definir quais serão os campos de pesquisa de cada termo.\n",
    "        O retorno deverá ser uma lista de fields.\n",
    "        Exemplo:\n",
    "        return ['title', 'abstract']\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def _db(self):\n",
    "        \"\"\"Cada subclasse deverá definir o seu banco.\n",
    "        Exemplo:\n",
    "        return 'pmc'\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def _sort_order(self):\n",
    "        \"\"\"Cada classe deverá definir o parâmetro sort_order.\n",
    "        Exemplo:\n",
    "        return 'Journal'\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def _article_url(self):\n",
    "        \"\"\"Cada classe deverá definir a URL da página de um artigo.\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class PMC_Searcher(NCBI_Searcher):\n",
    "    \"\"\"Realiza pesquisas na base PMC.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def _fields(self):\n",
    "        return ['Abstract', 'Body - Key Terms', 'MeSH Terms',\n",
    "                'MeSH Major Topic', 'Methods - Key Terms']\n",
    "\n",
    "    @property\n",
    "    def _db(self):\n",
    "        return 'pmc'\n",
    "\n",
    "    @property\n",
    "    def _sort_order(self):\n",
    "        return 'relevance'\n",
    "\n",
    "    @property\n",
    "    def _article_url(self):\n",
    "        return 'https://www.ncbi.nlm.nih.gov/pmc/articles/'\n",
    "\n",
    "    def _get_article_metadata(self, *args):\n",
    "        id_list = ','.join([str(x) for x in args])\n",
    "\n",
    "        payload = {\"id\": id_list, \"db\": self._db, \"retmode\": \"xml\"}\n",
    "        url = \"%s?%s\" % (self.fetch_url, urlencode(payload))\n",
    "        print(url)\n",
    "\n",
    "        soup = bsoup(requests.get(url).content, \"xml\")\n",
    "\n",
    "        pmc_articles = soup.findAll('article')\n",
    "\n",
    "        documentos = []\n",
    "        append = documentos.append\n",
    "\n",
    "        for p_art in pmc_articles:\n",
    "            author_list = p_art.findAll(\"contrib\", {\"contrib-type\": \"author\"})\n",
    "            authors = []\n",
    "            for author in author_list:\n",
    "                try:\n",
    "                    authors.append(\"%s %s\" % (getattr(author, \"given-names\").text, author.surname.text))\n",
    "                except:\n",
    "                    authors.append(author.text)\n",
    "\n",
    "            keywords = [k.text for k in p_art.findAll(\"kwd\")]\n",
    "            pmc_id = soup.findAll(\"article-id\", {\"pub-id-type\": 'pmc'})[0].text\n",
    "\n",
    "            documento = {}\n",
    "            documento['resumo'] = getattr(p_art.abstract, 'text', ' - ')\n",
    "            documento['html_url'] = \"%s%s\" % (self._article_url, pmc_id)\n",
    "            documento['autores'] = \",\".join(authors)\n",
    "            documento['doi'] = p_art.findAll(\"article-id\", {\"pub-id-type\": \"doi\"})[0].text\n",
    "            documento['palavras_chaves'] = \",\".join(keywords)\n",
    "            documento['titulo'] = getattr(p_art, \"article-title\").text\n",
    "\n",
    "            try:\n",
    "                pub_date = p_art.findAll(\"pub-date\", {\"pub-type\": \"epub\"})[0]\n",
    "            except:\n",
    "                pub_date = p_art.findAll(\"pub-date\", {\"pub-type\": \"ppub\"})[0]\n",
    "\n",
    "            data_pub_string = \"%s %s\" % (pub_date.year.text, pub_date.month.text)\n",
    "            documento['data'] = datetime.strptime(data_pub_string, \"%Y %m\").date()\n",
    "\n",
    "            append(documento)\n",
    "\n",
    "        return documentos\n",
    "\n",
    "\n",
    "class PubMed_Searcher(NCBI_Searcher):\n",
    "    \"\"\"Realiza pesquisas na base PubMed.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def _fields(self):\n",
    "        return ['Text Words']\n",
    "\n",
    "    @property\n",
    "    def _db(self):\n",
    "        return 'pubmed'\n",
    "\n",
    "    @property\n",
    "    def _sort_order(self):\n",
    "        return ''\n",
    "\n",
    "    @property\n",
    "    def _article_url(self):\n",
    "        return \"https://www.ncbi.nlm.nih.gov/pubmed/\"\n",
    "\n",
    "    def _get_article_metadata(self, *args):\n",
    "        id_list = ','.join([str(x) for x in args])\n",
    "\n",
    "        payload = {\"id\": id_list, \"db\": self._db, \"retmode\": \"xml\"}\n",
    "        url = \"%s?%s\" % (self.fetch_url, urlencode(payload))\n",
    "\n",
    "        soup = bsoup(requests.get(url).content, \"xml\")\n",
    "\n",
    "        pubmed_articles = soup.findAll('PubmedArticle')\n",
    "\n",
    "        documentos = []\n",
    "        append = documentos.append\n",
    "\n",
    "        for p_art in pubmed_articles:\n",
    "            authors = [\"%s %s\" % (a.ForeName.text, a.LastName.text) for a in p_art.findAll(\"Author\")]\n",
    "            keywords = [k.text for k in p_art.findAll(\"Keyword\")]\n",
    "            data_pub_string = \"%s %s\" % (\n",
    "            p_art.PubDate.Year.text, self.deepgetter(p_art, 'PubDate.Month.text', default='Jan'))\n",
    "\n",
    "            documento = {}\n",
    "            documento['resumo'] = getattr(p_art.AbstractText, 'text', ' - ')\n",
    "            documento['html_url'] = \"%s%s\" % (self._article_url, p_art.PMID.text)\n",
    "            documento['autores'] = \",\".join(authors)\n",
    "            documento['doi'] = p_art.findAll(\"ArticleId\", {\"IdType\": \"doi\"})[0].text\n",
    "            documento['palavras_chaves'] = \",\".join(keywords)\n",
    "            documento['data'] = datetime.strptime(data_pub_string, \"%Y %b\").date()\n",
    "            documento['titulo'] = p_art.ArticleTitle.text\n",
    "            append(documento)\n",
    "\n",
    "        return documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Você pode realizar essa mesma busca no navegador com o termo de busca:\n",
      "((machine learning OR deep learning OR artificial intelligence OR neural network OR scoring system) AND (coronary artery disease OR chest pain OR heart disease OR MACE OR Acute Cardiac Complications)) AND (\"BioMedical Engineering OnLine\"[Journal] OR \"Biomedical Engineering\"[Journal])\n",
      "QTD. resultados: 77\n",
      "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?id=4496820%2C5408446%2C3045988%2C2781013%2C4105825&db=pmc&retmode=xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The electronic stethoscope',\n",
       " 'Detecting central fixation by means of artificial ',\n",
       " 'A framework for automatic heart sound analysis wit',\n",
       " 'Robust algorithm for arrhythmia classification in ',\n",
       " 'Machine learning, medical diagnosis, and biomedica']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# termos de pesquisa relacionados a tecnologia\n",
    "technology_queryterms = [\n",
    "    'machine learning', 'deep learning', 'artificial intelligence', \n",
    "    'neural network', 'scoring system'\n",
    "]\n",
    "\n",
    "# termos de pesquisa relacionados a area da saude\n",
    "health_queryterms = [\n",
    "    'coronary artery disease', 'chest pain', 'heart disease', 'MACE', \n",
    "    'Acute Cardiac Complications'\n",
    "]\n",
    "\n",
    "queryterms = [technology_queryterms, health_queryterms]\n",
    "\n",
    "# lista de revistas que delimitam a busca\n",
    "journal = [\"BioMedical Engineering OnLine\",\n",
    "           \"Biomedical Engineering\"]\n",
    "\n",
    "r = PMC_Searcher().search(queryterms=queryterms, start_year=2008, max_records=5, journal=journal)\n",
    "[a['titulo'][:50] for a in r]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = PubMed_Searcher().search(queryterms=queryterms, start_year=2008, max_records=5, journal=journal)\n",
    "[a['titulo'][:50] for a in r]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
