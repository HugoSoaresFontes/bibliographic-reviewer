{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod\n",
    "from urllib.parse import urlencode\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup as bsoup\n",
    "from functools import reduce\n",
    "\n",
    "class NCBI_Searcher(metaclass=ABCMeta):\n",
    "    \"\"\" 'Interface' que define a utilização da API das databases da NCBI.\n",
    "    \"\"\"\n",
    "\n",
    "    search_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi'\n",
    "    meta_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi'\n",
    "    fetch_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi'\n",
    "\n",
    "    def search(self, queryterms: list = None, search_type: str = None,\n",
    "               start_year: int = 1900, end_year: int = None,\n",
    "               max_records: int = 20, start_record: int = 0,\n",
    "               author: str = None, journal: str = None):\n",
    "        \"\"\"\n",
    "        Realiza uma pesquisa NCBI.\n",
    "        @param queryterms: list of lists. Terms within the same list are\n",
    "            separated by an OR. Lists are separated by an AND\n",
    "        @param search_type: meta_data or querytext.\n",
    "            meta_data: This field enables a free-text search of all\n",
    "                configured metadata fields and the abstract.\n",
    "            querytext: This field enables a free-text search of all\n",
    "                fields.\n",
    "        @param start_year: Start value of Publication Year to restrict results by.\n",
    "        @param end_year: End value of Publication Year to restrict results by.\n",
    "        @param max_records: The number of records to fetch.\n",
    "        @param start_record: Sequence number of first record to fetch.\n",
    "        @param author: An author's name. Searches both first name and last name\n",
    "            Accepts a list of author names too.\n",
    "        @param journal: An author's name. Accepts a list of journals too.\n",
    "        @return: a dictionaries list whose keys are compatible with Documento model.\n",
    "        \"\"\"\n",
    "\n",
    "        term = self._search_term(queryterms, search_type=search_type)\n",
    "        if author:\n",
    "            author = [author] if type(author) == str else author\n",
    "            author = ['%s[Author]' % a for a in author]\n",
    "            term = \"%s AND (%s)\" % (term, \" OR \".join(author) )\n",
    "\n",
    "        if journal:\n",
    "            journal = [journal] if type(journal) == str else journal\n",
    "            journal = ['\"%s\"[Journal]' % j for j in journal]\n",
    "            term = \"%s AND (%s)\" % (term, \" OR \".join(journal) )\n",
    "\n",
    "        fixed_payload = {\"retmode\": \"json\", \"datetype\": \"pdat\",\n",
    "                         \"db\": self._db, \"sort\": self._sort_order}\n",
    "        payload = {\"term\": term,\n",
    "                   \"retmax\": max_records, \"retstart\": start_record,\n",
    "                   \"mindate\": start_year or '', \"maxdate\": end_year or datetime.now().year}\n",
    "        payload.update(fixed_payload)\n",
    "\n",
    "        url = \"%s?%s\" % (self.search_url, urlencode(payload))\n",
    "        \n",
    "        print(\"URL SEARCH: %s\" % url)\n",
    "\n",
    "        print(\"Você pode realizar essa mesma busca no navegador com o termo de busca:\\n%s\" % term)\n",
    "\n",
    "        response = requests.get(url).json()['esearchresult']\n",
    "\n",
    "        print('QTD. resultados: %s' % response['count'])\n",
    "\n",
    "        id_list = response['idlist']\n",
    "\n",
    "        if id_list:\n",
    "            return self._get_article_metadata(*id_list)\n",
    "        return []\n",
    "\n",
    "    def _search_term(self, queryterms: list, search_type: str = None):\n",
    "        \"\"\"Monta o termo de pesquisa completo para mandar para a API.\"\"\"\n",
    "\n",
    "        if search_type in ['querytext', None]:\n",
    "            # Retorna simplesmente a busca concatenando com os OR's e AND's\n",
    "            return \"(%s)\" % \" AND \".join([\"(%s)\" % \" OR \".join(orses) for orses in queryterms])\n",
    "        elif search_type != 'meta_data':\n",
    "            raise Exception('Tipo de pesquisa não faz sentido: %s\\nTipos suportados:' % search_type)\n",
    "\n",
    "        # Retorna concacentando com os OR'S e AND's, mas embutindo também os campos de pesquisa em cada termo\n",
    "        queryterms = [[self._embutir_fields(orses) for orses in andes] for andes in queryterms]\n",
    "        return \"(%s)\" % \" AND \".join([\"(%s)\" % \" OR \".join(orses) for orses in queryterms])\n",
    "\n",
    "    def _embutir_fields(self, term: str):\n",
    "        \"\"\"Faz uma transformação, embutindo fields no termo de pesquisa.\n",
    "        Isso é para poder realizar a pesquisa em apenas alguns campos ao invés de todos.\n",
    "        Exemplo: sendo self.__fields = ['title', 'abstract'],\n",
    "        a chamada\n",
    "        `self._embutir_fields(\"machine learning\")`\n",
    "        Transforma:\n",
    "            machine learning ---> (machine learning[title] OR machine learning[abstract])\n",
    "        \"\"\"\n",
    "\n",
    "        return \"(%s)\" % \" OR \".join([\"%s[%s]\" % (term, field) for field in self._fields])\n",
    "\n",
    "    @staticmethod\n",
    "    def deepgetter(obj, attrs, default=None):\n",
    "        \"\"\"Faz uma chamada sucessiva da função getattr, para ir pegando os atributos\n",
    "        de um objeto.\n",
    "        Exemplo:\n",
    "        deepgetter(Cidade, 'regiao.pais') é equivalente a fazer Cidade.regiao.pais\n",
    "        \"\"\"\n",
    "        getter = lambda x, y: getattr(x, y, default)\n",
    "        return reduce(getter, attrs.split('.'), obj)\n",
    "\n",
    "    @abstractmethod\n",
    "    def _get_article_metadata(self, *args):\n",
    "        \"\"\"Cada subclasse deverá implementar a função que pega o retorno da API e transforma numa lista de dicionários\n",
    "        no formato do modelo Documento.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def _fields(self):\n",
    "        \"\"\"Cada subclasse deverá definir quais serão os campos de pesquisa de cada termo.\n",
    "        O retorno deverá ser uma lista de fields.\n",
    "        Exemplo:\n",
    "        return ['title', 'abstract']\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def _db(self):\n",
    "        \"\"\"Cada subclasse deverá definir o seu banco.\n",
    "        Exemplo:\n",
    "        return 'pmc'\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def _sort_order(self):\n",
    "        \"\"\"Cada classe deverá definir o parâmetro sort_order.\n",
    "        Exemplo:\n",
    "        return 'Journal'\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def _article_url(self):\n",
    "        \"\"\"Cada classe deverá definir a URL da página de um artigo.\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class PMC_Searcher(NCBI_Searcher):\n",
    "    \"\"\"Realiza pesquisas na base PMC.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def _fields(self):\n",
    "        return ['Abstract', 'Body - Key Terms', 'MeSH Terms',\n",
    "                'MeSH Major Topic', 'Methods - Key Terms']\n",
    "\n",
    "    @property\n",
    "    def _db(self):\n",
    "        return 'pmc'\n",
    "\n",
    "    @property\n",
    "    def _sort_order(self):\n",
    "        return 'relevance'\n",
    "\n",
    "    @property\n",
    "    def _article_url(self):\n",
    "        return 'https://www.ncbi.nlm.nih.gov/pmc/articles/'\n",
    "\n",
    "    def _get_article_metadata(self, *args):\n",
    "        id_list = ','.join([str(x) for x in args])\n",
    "\n",
    "        payload = {\"id\": id_list, \"db\": self._db, \"retmode\": \"xml\"}\n",
    "        url = \"%s?%s\" % (self.fetch_url, urlencode(payload))\n",
    "        print(\"URL META: %s\" % url)\n",
    "\n",
    "        soup = bsoup(requests.get(url).content, \"xml\")\n",
    "\n",
    "        pmc_articles = soup.findAll('article')\n",
    "\n",
    "        documentos = []\n",
    "        append = documentos.append\n",
    "\n",
    "        for p_art in pmc_articles:\n",
    "            author_list = p_art.findAll(\"contrib\", {\"contrib-type\": \"author\"})\n",
    "            authors = []\n",
    "            for author in author_list:\n",
    "                try:\n",
    "                    authors.append(\"%s %s\" % (getattr(author, \"given-names\").text, author.surname.text))\n",
    "                except:\n",
    "                    authors.append(author.text)\n",
    "\n",
    "            keywords = [k.text for k in p_art.findAll(\"kwd\")]\n",
    "            pmc_id = soup.findAll(\"article-id\", {\"pub-id-type\": 'pmc'})[0].text\n",
    "\n",
    "            documento = {}\n",
    "            documento['resumo'] = getattr(p_art.abstract, 'text', ' - ')\n",
    "            documento['html_url'] = \"%s%s\" % (self._article_url, pmc_id)\n",
    "            documento['autores'] = \",\".join(authors)\n",
    "            documento['doi'] = p_art.findAll(\"article-id\", {\"pub-id-type\": \"doi\"})[0].text\n",
    "            documento['palavras_chaves'] = \",\".join(keywords)\n",
    "            documento['titulo'] = getattr(p_art, \"article-title\").text\n",
    "\n",
    "            try:\n",
    "                pub_date = p_art.findAll(\"pub-date\", {\"pub-type\": \"epub\"})[0]\n",
    "            except:\n",
    "                pub_date = p_art.findAll(\"pub-date\", {\"pub-type\": \"ppub\"})[0]\n",
    "\n",
    "            data_pub_string = \"%s %s\" % (pub_date.year.text, pub_date.month.text)\n",
    "            documento['data'] = datetime.strptime(data_pub_string, \"%Y %m\").date()\n",
    "\n",
    "            append(documento)\n",
    "\n",
    "        return documentos\n",
    "\n",
    "\n",
    "class PubMed_Searcher(NCBI_Searcher):\n",
    "    \"\"\"Realiza pesquisas na base PubMed.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def _fields(self):\n",
    "        return ['Text Words']\n",
    "\n",
    "    @property\n",
    "    def _db(self):\n",
    "        return 'pubmed'\n",
    "\n",
    "    @property\n",
    "    def _sort_order(self):\n",
    "        return ''\n",
    "\n",
    "    @property\n",
    "    def _article_url(self):\n",
    "        return \"https://www.ncbi.nlm.nih.gov/pubmed/\"\n",
    "\n",
    "    def _get_article_metadata(self, *args):\n",
    "        id_list = ','.join([str(x) for x in args])\n",
    "\n",
    "        payload = {\"id\": id_list, \"db\": self._db, \"retmode\": \"xml\"}\n",
    "        url = \"%s?%s\" % (self.fetch_url, urlencode(payload))\n",
    "        \n",
    "        print(\"URL META: %s\" % url)\n",
    "\n",
    "        soup = bsoup(requests.get(url).content, \"xml\")\n",
    "\n",
    "        pubmed_articles = soup.findAll('PubmedArticle')\n",
    "\n",
    "        documentos = []\n",
    "        append = documentos.append\n",
    "\n",
    "        for p_art in pubmed_articles:\n",
    "            authors = [\"%s %s\" % (a.ForeName.text, a.LastName.text) for a in p_art.findAll(\"Author\")]\n",
    "            keywords = [k.text for k in p_art.findAll(\"Keyword\")]\n",
    "            data_pub_string = \"%s %s\" % (\n",
    "            p_art.PubDate.Year.text, self.deepgetter(p_art, 'PubDate.Month.text', default='Jan'))\n",
    "\n",
    "            documento = {}\n",
    "            documento['resumo'] = getattr(p_art.AbstractText, 'text', ' - ')\n",
    "            documento['html_url'] = \"%s%s\" % (self._article_url, p_art.PMID.text)\n",
    "            documento['autores'] = \",\".join(authors)\n",
    "            documento['doi'] = p_art.findAll(\"ArticleId\", {\"IdType\": \"doi\"})[0].text\n",
    "            documento['palavras_chaves'] = \",\".join(keywords)\n",
    "            documento['data'] = datetime.strptime(data_pub_string, \"%Y %b\").date()\n",
    "            documento['titulo'] = p_art.ArticleTitle.text\n",
    "            append(documento)\n",
    "\n",
    "        return documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# termos de pesquisa relacionados a tecnologia\n",
    "technology_queryterms = [\n",
    "    'machine learning', 'deep learning', 'artificial intelligence', \n",
    "    'neural network', 'scoring system'\n",
    "]\n",
    "\n",
    "# termos de pesquisa relacionados a area da saude\n",
    "health_queryterms = [\n",
    "    'coronary artery disease', 'chest pain', 'heart disease', 'MACE', \n",
    "    'Acute Cardiac Complications'\n",
    "]\n",
    "\n",
    "queryterms = [technology_queryterms, health_queryterms]\n",
    "\n",
    "# lista de revistas que delimitam a busca\n",
    "journal = [\"BioMedical Engineering OnLine\",\n",
    "           \"Biomedical Engineering\"]\n",
    "\n",
    "# r = PMC_Searcher().search(queryterms=queryterms, start_year=2008, max_records=5, journal=journal)\n",
    "# [a['titulo'][:50] for a in r]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL SEARCH: https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?term=%28%28machine+learning+OR+deep+learning+OR+artificial+intelligence+OR+neural+network+OR+scoring+system%29+AND+%28coronary+artery+disease+OR+chest+pain+OR+heart+disease+OR+MACE+OR+Acute+Cardiac+Complications%29%29+AND+%28%22BioMedical+Engineering+OnLine%22%5BJournal%5D+OR+%22Biomedical+Engineering%22%5BJournal%5D%29&retmax=5&retstart=0&mindate=2008&maxdate=2018&retmode=json&datetype=pdat&db=pubmed&sort=\n",
      "Você pode realizar essa mesma busca no navegador com o termo de busca:\n",
      "((machine learning OR deep learning OR artificial intelligence OR neural network OR scoring system) AND (coronary artery disease OR chest pain OR heart disease OR MACE OR Acute Cardiac Complications)) AND (\"BioMedical Engineering OnLine\"[Journal] OR \"Biomedical Engineering\"[Journal])\n",
      "QTD. resultados: 10\n",
      "URL META: https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?id=26329721%2C26159433%2C26126807%2C25491135%2C24981916&db=pubmed&retmode=xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['cMRI-BED: A novel informatics framework for cardia',\n",
       " 'The electronic stethoscope.',\n",
       " 'Analysis of short-term heart rate and diastolic pe',\n",
       " 'Implementation of a portable device for real-time ',\n",
       " 'A new hierarchical method for inter-patient heartb']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = PubMed_Searcher().search(queryterms=queryterms, start_year=2008, max_records=5, journal=journal)\n",
    "[a['titulo'][:50] for a in r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL SEARCH: https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?term=%28%28machine+learning+OR+deep+learning+OR+artificial+intelligence+OR+neural+network+OR+scoring+system%29+AND+%28coronary+artery+disease+OR+chest+pain+OR+heart+disease+OR+MACE+OR+Acute+Cardiac+Complications%29%29+AND+%28%22BioMedical+Engineering+OnLine%22%5BJournal%5D+OR+%22Biomedical+Engineering%22%5BJournal%5D%29&retmax=5&retstart=0&mindate=1900&maxdate=2018&retmode=json&datetype=pdat&db=pmc&sort=relevance\n",
      "Você pode realizar essa mesma busca no navegador com o termo de busca:\n",
      "((machine learning OR deep learning OR artificial intelligence OR neural network OR scoring system) AND (coronary artery disease OR chest pain OR heart disease OR MACE OR Acute Cardiac Complications)) AND (\"BioMedical Engineering OnLine\"[Journal] OR \"Biomedical Engineering\"[Journal])\n",
      "QTD. resultados: 88\n",
      "URL META: https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?id=4496820%2C5408446%2C3045988%2C2781013%2C4105825&db=pmc&retmode=xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'autores': 'Shuang Leng,Ru San Tan,Kevin Tshun Chuan Chai,Chao Wang,Dhanjoo Ghista,Liang Zhong',\n",
       "  'data': datetime.date(2015, 7, 1),\n",
       "  'doi': '10.1186/s12938-015-0056-y',\n",
       "  'html_url': 'https://www.ncbi.nlm.nih.gov/pmc/articles/4496820',\n",
       "  'palavras_chaves': 'Heart sound,Heart auscultation,Heart disorder,Diagnosis,Acoustic technique,Automatic system,Smartphone stethoscope apps',\n",
       "  'resumo': '\\nMost heart diseases are associated with and reflected by the sounds that the heart produces. Heart auscultation, defined as\\nlistening to the heart sound, has been a very important method for the early diagnosis of cardiac dysfunction. Traditional auscultation requires substantial clinical experience and good listening skills. The emergence of the electronic stethoscope has paved the way for a new field of computer-aided auscultation. This article provides an in-depth study of (1) the electronic stethoscope technology, and (2) the methodology for diagnosis of cardiac disorders based on computer-aided auscultation. The paper is based on a comprehensive review of (1) literature articles, (2) market (state-of-the-art) products, and (3) smartphone stethoscope apps. It covers in depth every key component of the computer-aided system with electronic stethoscope, from sensor design, front-end circuitry, denoising algorithm, heart sound segmentation, to the final machine learning techniques. Our intent is to provide an informative and illustrative presentation of the electronic stethoscope, which is valuable and beneficial to academics, researchers and engineers in the technical field, as well as to medical professionals to facilitate its use clinically. The paper provides the technological and medical basis for the development and commercialization of a real-time integrated heart sound detection, acquisition and quantification system.\\n',\n",
       "  'titulo': 'The electronic stethoscope'},\n",
       " {'autores': 'Boris I. Gramatikov',\n",
       "  'data': datetime.date(2017, 4, 1),\n",
       "  'doi': '10.1186/s12938-017-0339-6',\n",
       "  'html_url': 'https://www.ncbi.nlm.nih.gov/pmc/articles/4496820',\n",
       "  'palavras_chaves': 'Artificial neural networks,Vision screener,Amblyopia,Strabismus,Fixation detection,Birefringence,Retina',\n",
       "  'resumo': '\\n\\nBackground\\nReliable detection of central fixation and eye alignment is essential in the diagnosis of amblyopia (“lazy eye”), which can lead to blindness. Our lab has developed and reported earlier a pediatric vision screener that performs scanning of the retina around the fovea and analyzes changes in the polarization state of light as the scan progresses. Depending on the direction of gaze and the instrument design, the screener produces several signal frequencies that can be utilized in the detection of central fixation. The objective of this study was to compare artificial neural networks with classical statistical methods, with respect to their ability to detect central fixation reliably.\\n\\n\\nMethods\\nA classical feedforward, pattern recognition, two-layer neural network architecture was used, consisting of one hidden layer and one output layer. The network has four inputs, representing normalized spectral powers at four signal frequencies generated during retinal birefringence scanning. The hidden layer contains four neurons. The output suggests presence or absence of central fixation. Backpropagation was used to train the network, using the gradient descent algorithm and the cross-entropy error as the performance function. The network was trained, validated and tested on a set of controlled calibration data obtained from 600 measurements from ten eyes in a previous study, and was additionally tested on a clinical set of 78 eyes, independently diagnosed by an ophthalmologist.\\n\\n\\nResults\\nIn the first part of this study, a neural network was designed around the calibration set. With a proper architecture and training, the network provided performance that was comparable to classical statistical methods, allowing perfect separation between the central and paracentral fixation data, with both the sensitivity and the specificity of the instrument being 100%. In the second part of the study, the neural network was applied to the clinical data. It allowed reliable separation between normal subjects and affected subjects, its accuracy again matching that of the statistical methods.\\n\\n\\nConclusion\\nWith a proper choice of a neural network architecture and a good, uncontaminated training data set, the artificial neural network can be an efficient classification tool for detecting central fixation based on retinal birefringence scanning.\\n\\n',\n",
       "  'titulo': 'Detecting central fixation by means of artificial neural networks in a pediatric vision screener using retinal birefringence scanning'},\n",
       " {'autores': 'Sumeth Yuenyong,Akinori Nishihara,Waree Kongprawechnon,Kanokvate Tungpimolrut',\n",
       "  'data': datetime.date(2011, 2, 1),\n",
       "  'doi': '10.1186/1475-925X-10-13',\n",
       "  'html_url': 'https://www.ncbi.nlm.nih.gov/pmc/articles/4496820',\n",
       "  'palavras_chaves': '',\n",
       "  'resumo': '\\n\\nBackground\\nA new framework for heart sound analysis is proposed. One of the most difficult processes in heart sound analysis is segmentation, due to interference form murmurs.\\n\\n\\nMethod\\nEqual number of cardiac cycles were extracted from heart sounds with different heart rates using information from envelopes of autocorrelation functions without the need to label individual fundamental heart sounds (FHS). The complete method consists of envelope detection, calculation of cardiac cycle lengths using auto-correlation of envelope signals, features extraction using discrete wavelet transform, principal component analysis, and classification using neural network bagging predictors.\\n\\n\\nResult\\nThe proposed method was tested on a set of heart sounds obtained from several on-line databases and recorded with an electronic stethoscope. Geometric mean was used as performance index. Average classification performance using ten-fold cross-validation was 0.92 for noise free case, 0.90 under white noise with 10 dB signal-to-noise ratio (SNR), and 0.90 under impulse noise up to 0.3 s duration.\\n\\n\\nConclusion\\nThe proposed method showed promising results and high noise robustness to a wide range of heart sounds. However, more tests are needed to address any bias that may have been introduced by different sources of heart sounds in the current training set, and to concretely validate the method. Further work include building a new training set recorded from actual patients, then further evaluate the method based on this new training set.\\n\\n',\n",
       "  'titulo': 'A framework for automatic heart sound analysis without segmentation'},\n",
       " {'autores': 'Jinkwon Kim,Hang Sik Shin,Kwangsoo Shin,Myoungho Lee',\n",
       "  'data': datetime.date(2009, 10, 1),\n",
       "  'doi': '10.1186/1475-925X-8-31',\n",
       "  'html_url': 'https://www.ncbi.nlm.nih.gov/pmc/articles/4496820',\n",
       "  'palavras_chaves': '',\n",
       "  'resumo': '\\n\\nBackground\\nRecently, extensive studies have been carried out on arrhythmia classification algorithms using artificial intelligence pattern recognition methods such as neural network. To improve practicality, many studies have focused on learning speed and the accuracy of neural networks. However, algorithms based on neural networks still have some problems concerning practical application, such as slow learning speeds and unstable performance caused by local minima.\\n\\n\\nMethods\\nIn this paper we propose a novel arrhythmia classification algorithm which has a fast learning speed and high accuracy, and uses Morphology Filtering, Principal Component Analysis and Extreme Learning Machine (ELM). The proposed algorithm can classify six beat types: normal beat, left bundle branch block, right bundle branch block, premature ventricular contraction, atrial premature beat, and paced beat.\\n\\n\\nResults\\nThe experimental results of the entire MIT-BIH arrhythmia database demonstrate that the performances of the proposed algorithm are 98.00% in terms of average sensitivity, 97.95% in terms of average specificity, and 98.72% in terms of average accuracy. These accuracy levels are higher than or comparable with those of existing methods. We make a comparative study of algorithm using an ELM, back propagation neural network (BPNN), radial basis function network (RBFN), or support vector machine (SVM). Concerning the aspect of learning time, the proposed algorithm using ELM is about 290, 70, and 3 times faster than an algorithm using a BPNN, RBFN, and SVM, respectively.\\n\\n\\nConclusion\\nThe proposed algorithm shows effective accuracy performance with a short learning time. In addition we ascertained the robustness of the proposed algorithm by evaluating the entire MIT-BIH arrhythmia database.\\n\\n',\n",
       "  'titulo': 'Robust algorithm for arrhythmia classification in ECG using extreme learning machine'},\n",
       " {'autores': 'Kenneth R Foster,Robert Koprowski,Joseph D Skufca',\n",
       "  'data': datetime.date(2014, 7, 1),\n",
       "  'doi': '10.1186/1475-925X-13-94',\n",
       "  'html_url': 'https://www.ncbi.nlm.nih.gov/pmc/articles/4496820',\n",
       "  'palavras_chaves': 'Artificial intelligence,Classifiers,Image processing,Machine learning,Support vector machine',\n",
       "  'resumo': '\\nA large number of papers are appearing in the biomedical engineering literature that describe the use of machine learning techniques to develop classifiers for detection or diagnosis of disease. However, the usefulness of this approach in developing clinically validated diagnostic techniques so far has been limited and the methods are prone to overfitting and other problems which may not be immediately apparent to the investigators. This commentary is intended to help sensitize investigators as well as readers and reviewers of papers to some potential pitfalls in the development of classifiers, and suggests steps that researchers can take to help avoid these problems. Building classifiers should be viewed not simply as an add-on statistical analysis, but as part and parcel of the experimental process. Validation of classifiers for diagnostic applications should be considered as part of a much larger process of establishing the clinical validity of the diagnostic technique.\\n',\n",
       "  'titulo': 'Machine learning, medical diagnosis, and biomedical engineering research - commentary'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PMC_Searcher().search(queryterms=queryterms, journal=journal, max_records=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL SEARCH: https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?term=%28%28machine+learning+OR+deep+learning+OR+artificial+intelligence+OR+neural+network+OR+scoring+system%29+AND+%28coronary+artery+disease+OR+chest+pain+OR+heart+disease+OR+MACE+OR+Acute+Cardiac+Complications%29%29&retmax=5&retstart=0&mindate=None&maxdate=2018&retmode=json&datetype=pdat&db=pmc&sort=relevance\n",
      "Você pode realizar essa mesma busca no navegador com o termo de busca:\n",
      "((machine learning OR deep learning OR artificial intelligence OR neural network OR scoring system) AND (coronary artery disease OR chest pain OR heart disease OR MACE OR Acute Cardiac Complications))\n",
      "QTD. resultados: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kwargs = {}\n",
    "PMC_Searcher().search(queryterms=queryterms, journal=kwargs.get('revistas'), start_year=kwargs.get('ano_inicio')\n",
    ", end_year=kwargs.get('ano_fim'), max_records=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oi'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = '' or 'oi'\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
