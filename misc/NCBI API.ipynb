{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod\n",
    "from urllib.parse import urlencode\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup as bsoup\n",
    "from functools import reduce\n",
    "import time\n",
    "import xmltodict, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "\n",
    "class ThreadWithReturnValue(Thread):\n",
    "    def __init__(self, group=None, target=None, name=None,\n",
    "                 args=(), kwargs=None, daemon=None):\n",
    "        Thread.__init__(self, group=group, target=target, name=name,\n",
    "                 args=args, kwargs=kwargs, daemon=daemon)\n",
    "        self._return = None\n",
    "    def run(self):\n",
    "        if self._target is not None:\n",
    "            self._return = self._target(*self._args,\n",
    "                                                **self._kwargs)\n",
    "    def join(self):\n",
    "        Thread.join(self)\n",
    "        return self._return\n",
    "\n",
    "    \n",
    "class NCBI_Searcher(metaclass=ABCMeta):\n",
    "    \"\"\" 'Interface' que define a utilização da API das databases da NCBI.\n",
    "    \"\"\"\n",
    "\n",
    "    search_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi'\n",
    "    meta_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi'\n",
    "    fetch_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi'\n",
    "    ncbi_register = {\"tool\":\"Atena\", \"email\":\"ddddiegolima@gmail.com\"}\n",
    "    max_pagination = 20\n",
    "    recursive = True\n",
    "\n",
    "    def search(self, queryterms: list = None, search_type: str = None,\n",
    "               start_year: int = None, end_year: int = None,\n",
    "               max_records: int = None, start_record: int = None,\n",
    "               author: str = None, journal: str = None, search_url: str = None):\n",
    "        \"\"\"\n",
    "        Realiza uma pesquisa NCBI.\n",
    "        @param queryterms: list of lists. Terms within the same list are\n",
    "            separated by an OR. Lists are separated by an AND\n",
    "        @param search_type: meta_data or querytext.\n",
    "            meta_data: This field enables a free-text search of all\n",
    "                configured metadata fields and the abstract.\n",
    "            querytext: This field enables a free-text search of all\n",
    "                fields.\n",
    "        @param start_year: Start value of Publication Year to restrict results by.\n",
    "        @param end_year: End value of Publication Year to restrict results by.\n",
    "        @param max_records: The number of records to fetch.\n",
    "        @param start_record: Sequence number of first record to fetch.\n",
    "        @param author: An author's name. Searches both first name and last name\n",
    "            Accepts a list of author names too.\n",
    "        @param journal: An author's name. Accepts a list of journals too.\n",
    "        @param search_url: Optionally you can directly specify the URL to \n",
    "            query from. Setting this parameter will ignore the other parameters.\n",
    "        @return: a dictionaries list whose keys are compatible with Documento model.\n",
    "        \"\"\"\n",
    "\n",
    "        term = self._search_term(queryterms, search_type=search_type)\n",
    "        if author:\n",
    "            author = [author] if type(author) == str else author\n",
    "            author = ['%s[Author]' % a for a in author]\n",
    "            term = \"%s AND (%s)\" % (term, \" OR \".join(author) )\n",
    "\n",
    "        if journal:\n",
    "            journal = [journal] if type(journal) == str else journal\n",
    "            journal = ['\"%s\"[Journal]' % j for j in journal]\n",
    "            term = \"%s AND (%s)\" % (term, \" OR \".join(journal) )\n",
    "\n",
    "        fixed_payload = {\"retmode\": \"json\", \"datetype\": \"pdat\",\n",
    "                         \"db\": self._db, \"sort\": self._sort_order}\n",
    "        payload = {\"term\": term,\n",
    "                   \"retmax\": max_records or '', \"retstart\": start_record or '',\n",
    "                   \"mindate\": start_year or '', \"maxdate\": end_year or ''}\n",
    "        payload.update(fixed_payload)\n",
    "        payload.update(self.ncbi_register)\n",
    "        url = search_url if search_url else \"%s?%s\" % (self.search_url, urlencode(payload))\n",
    "        \n",
    "#         print(\"URL SEARCH: %s\" % url)\n",
    "        t_00 = time.time()\n",
    "        response = requests.get(url).json()['esearchresult']\n",
    "        print('{:15s}{:6.3f}'.format(\"response\",time.time() - t_00))\n",
    "        quantidade_artigos = int(response['count'])\n",
    "        if self.recursive:\n",
    "            print(\"Artigos encontrados: \",quantidade_artigos)\n",
    "        # Se o usuário não limitou quantidade de resultados, então traz tudo\n",
    "        max_records = max_records or quantidade_artigos\n",
    "        \n",
    "        retorno = []\n",
    "        \n",
    "        ###\n",
    "        ### BLOCO FANTASMA\n",
    "        ###\n",
    "        # Se houver necessidade de paginação...\n",
    "#         if quantidade_artigos > self.max_pagination and max_records > self.max_pagination:\n",
    "#             # self.recursive só sera True se a chamada estiver sendo feita pelo usuário.\n",
    "#             # Isso serve para garantir que cada chamada da função self.search\n",
    "#             # neste bloco só acontecerá com um nível de recursividade.\n",
    "#             if self.recursive:\n",
    "#                 self.recursive = False\n",
    "                \n",
    "#                 threads = []\n",
    "#                 payload.update({'retmax':self.max_pagination})\n",
    "#                 for i,x in enumerate(range(20, quantidade_artigos+1, self.max_pagination)):\n",
    "                    \n",
    "#                     payload.update({'retstart':x})\n",
    "#                     kwargs = {\"search_url\": \"%s?%s\" % (self.search_url, urlencode(payload))}\n",
    "                    \n",
    "#                     thread = ThreadWithReturnValue(target=self.search, kwargs=kwargs)\n",
    "#                     threads.append(thread)\n",
    "#                     thread.start()\n",
    "                    \n",
    "#                     if (i+1)%3==0:\n",
    "#                         print(\"sleeping\")\n",
    "#                         time.sleep(2)\n",
    "\n",
    "#                 for thread in threads:\n",
    "#                     lista = thread.join()\n",
    "#                     print(\"thread fetching \",len(lista))\n",
    "#                     retorno.extend(lista)\n",
    "                    \n",
    "#                 self.recursive = True\n",
    "                \n",
    "        ###\n",
    "        ### FIM BLOCO FANTASMA\n",
    "        ###\n",
    "                    \n",
    "\n",
    "        id_list = response['idlist']\n",
    "\n",
    "        if id_list:\n",
    "            lista = self._get_article_metadata(*id_list)\n",
    "            retorno.extend(lista)\n",
    "        return retorno\n",
    "\n",
    "    def _search_term(self, queryterms: list, search_type: str = None):\n",
    "        \"\"\"Monta o termo de pesquisa completo para mandar para a API.\"\"\"\n",
    "        \n",
    "        if type(queryterms) != list:\n",
    "            return\n",
    "\n",
    "        if search_type in ['querytext', None]:\n",
    "            # Retorna simplesmente a busca concatenando com os OR's e AND's\n",
    "            return \"(%s)\" % \" AND \".join([\"(%s)\" % \" OR \".join(orses) for orses in queryterms])\n",
    "        elif search_type != 'meta_data':\n",
    "            raise Exception('Tipo de pesquisa não faz sentido: %s\\nTipos suportados:' % search_type)\n",
    "\n",
    "        # Retorna concacentando com os OR'S e AND's, mas embutindo também os campos de pesquisa em cada termo\n",
    "        queryterms = [[self._embutir_fields(orses) for orses in andes] for andes in queryterms]\n",
    "        return \"(%s)\" % \" AND \".join([\"(%s)\" % \" OR \".join(orses) for orses in queryterms])\n",
    "\n",
    "    def _embutir_fields(self, term: str):\n",
    "        \"\"\"Faz uma transformação, embutindo fields no termo de pesquisa.\n",
    "        Isso é para poder realizar a pesquisa em apenas alguns campos ao invés de todos.\n",
    "        Exemplo: sendo self.__fields = ['title', 'abstract'],\n",
    "        a chamada\n",
    "        `self._embutir_fields(\"machine learning\")`\n",
    "        Transforma:\n",
    "            machine learning ---> (machine learning[title] OR machine learning[abstract])\n",
    "        \"\"\"\n",
    "\n",
    "        return \"(%s)\" % \" OR \".join([\"%s[%s]\" % (term, field) for field in self._fields])\n",
    "\n",
    "    @staticmethod\n",
    "    def deepgetter(obj, attrs, default=None):\n",
    "        \"\"\"Faz uma chamada sucessiva da função getattr, para ir pegando os atributos\n",
    "        de um objeto.\n",
    "        Exemplo:\n",
    "        deepgetter(Cidade, 'regiao.pais') é equivalente a fazer Cidade.regiao.pais\n",
    "        \"\"\"\n",
    "        getter = lambda x, y: getattr(x, y, default)\n",
    "        return reduce(getter, attrs.split('.'), obj)\n",
    "\n",
    "    @abstractmethod\n",
    "    def _get_article_metadata(self, *args):\n",
    "        \"\"\"Cada subclasse deverá implementar a função que pega o retorno da API e transforma numa lista de dicionários\n",
    "        no formato do modelo Documento.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def _fields(self):\n",
    "        \"\"\"Cada subclasse deverá definir quais serão os campos de pesquisa de cada termo.\n",
    "        O retorno deverá ser uma lista de fields.\n",
    "        Exemplo:\n",
    "        return ['title', 'abstract']\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def _db(self):\n",
    "        \"\"\"Cada subclasse deverá definir o seu banco.\n",
    "        Exemplo:\n",
    "        return 'pmc'\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def _sort_order(self):\n",
    "        \"\"\"Cada classe deverá definir o parâmetro sort_order.\n",
    "        Exemplo:\n",
    "        return 'Journal'\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def _article_url(self):\n",
    "        \"\"\"Cada classe deverá definir a URL da página de um artigo.\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class PMC_Searcher(NCBI_Searcher):\n",
    "    \"\"\"Realiza pesquisas na base PMC.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def _fields(self):\n",
    "        return ['Abstract', 'Body - Key Terms', 'MeSH Terms',\n",
    "                'MeSH Major Topic', 'Methods - Key Terms']\n",
    "\n",
    "    @property\n",
    "    def _db(self):\n",
    "        return 'pmc'\n",
    "\n",
    "    @property\n",
    "    def _sort_order(self):\n",
    "        return 'relevance'\n",
    "\n",
    "    @property\n",
    "    def _article_url(self):\n",
    "        return 'https://www.ncbi.nlm.nih.gov/pmc/articles/'\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_data(p_art):\n",
    "        \"\"\"Vasculha o XML (um <PubmedArticle>) para encontrar a data de publicação\n",
    "        Se for encontrada uma data válida, retorna um datetime.\n",
    "        Se não, retorna uma string, que espera-se que contenha uma informação de data\"\"\"\n",
    "\n",
    "        try:\n",
    "            pub_date = p_art.findAll(\"pub-date\", {\"pub-type\": \"epub\"})[0]\n",
    "        except:\n",
    "            pub_date = p_art.findAll(\"pub-date\", {\"pub-type\": \"ppub\"})[0]\n",
    "\n",
    "        data_pub_string = \"%s %s\" % (pub_date.year.text, NCBI_Searcher.deepgetter(pub_date, 'month.text', default='Jan'))\n",
    "\n",
    "        try:\n",
    "            data = datetime.strptime(data_pub_string, \"%Y %m\").date()\n",
    "        except:\n",
    "            try:\n",
    "                data = datetime.strptime(data_pub_string, \"%Y %b\").date()\n",
    "            except:\n",
    "                data = data_pub_string\n",
    "\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_unique_id(p_art):\n",
    "        \"\"\"Vascula o XML (um <PubmedArticle>) para encontrar o ID único do artigo.\n",
    "        Se nao tiver DOI presente no XML, coloca o ID que tiver (esperado que seja o PubMed ID)\"\"\"\n",
    "\n",
    "        try:\n",
    "            unique_id = p_art.findAll(\"article-id\", {\"pub-id-type\": \"doi\"})[0].text\n",
    "        except:\n",
    "            unique_id = p_art.findAll(\"article-id\")[0]\n",
    "            unique_id = \"%s%s\" % (unique_id['pub-id-type'], unique_id.text)\n",
    "\n",
    "        return unique_id\n",
    "\n",
    "    def _get_article_metadata(self, *args):\n",
    "        id_list = ','.join([str(x) for x in args])\n",
    "\n",
    "        payload = {\"id\": id_list, \"db\": self._db, \"retmode\": \"xml\"}\n",
    "        payload.update(self.ncbi_register)\n",
    "        url = \"%s?%s\" % (self.fetch_url, urlencode(payload))\n",
    "        print(\"URL META: %s\" % url)\n",
    "\n",
    "        t_05 = time.time()\n",
    "        r = requests.get(url)\n",
    "        print('{:15s}{:6.3f}'.format(\"response_M\",time.time() - t_05))\n",
    "        \n",
    "        t_02 = time.time()\n",
    "        # Pegar o XML, e transformar num dicionário\n",
    "        d = json.loads(json.dumps(xmltodict.parse(r.content)))\n",
    "        articles = d['pmc-articleset']['article']\n",
    "        print('{:15s}{:6.3f}'.format(\"parse\",time.time() - t_02))\n",
    "\n",
    "        documentos = []\n",
    "        append = documentos.append\n",
    "\n",
    "        t_04 = time.time()\n",
    "        debug = True\n",
    "        for article in articles:\n",
    "            ### TITULO\n",
    "            try:\n",
    "                title = article['front']['article-meta']['title-group']['article-title']\n",
    "            except Exception as e:\n",
    "                title = ''\n",
    "                print(e.__class__.__name__,e)\n",
    "                if debug:\n",
    "                    ipdb.set_trace()\n",
    "\n",
    "            ### AUTORES\n",
    "            try:\n",
    "                authors = []\n",
    "                for contrib in article['front']['article-meta']['contrib-group']['contrib']:\n",
    "                    try:\n",
    "                        authors.append(\"%s %s\" % (contrib['name']['given-names'], contrib['name']['surname']))\n",
    "                    except:\n",
    "                        pass\n",
    "            except Exception as e:\n",
    "                authors = []\n",
    "                print(e.__class__.__name__,e)\n",
    "                if debug:\n",
    "                    ipdb.set_trace()\n",
    "\n",
    "            ### PALAVRAS CHAVE \n",
    "            try:    \n",
    "                palavras_chave = [k if type(k)==str else k['#text']for k in article['front']['article-meta']['kwd-group']['kwd']]\n",
    "            except Exception as e:\n",
    "                palavras_chave = []\n",
    "                print(e.__class__.__name__,e)\n",
    "                if debug:\n",
    "                    ipdb.set_trace()\n",
    "\n",
    "            ### DOI / IDs\n",
    "            try:\n",
    "                doi = [id['#text'] for id in article['front']['article-meta']['article-id'] if  id['@pub-id-type'] == 'doi'][-1] or ''\n",
    "                pmc_id = [id['#text'] for id in article['front']['article-meta']['article-id'] if  id['@pub-id-type'] == 'pmc'][-1] or ''\n",
    "            except Exception as e:\n",
    "                doi = ''\n",
    "                pmc_id = ''\n",
    "                print(e.__class__.__name__,e)\n",
    "                if debug:\n",
    "                    ipdb.set_trace()\n",
    "\n",
    "            ### Abstract\n",
    "            try:\n",
    "                abstract = article['front']['article-meta']['abstract']\n",
    "                if type(abstract) == dict:\n",
    "                    resumo = abstract['p']['#text']\n",
    "                elif type(abstract) == list:\n",
    "                    for ab in abstract:\n",
    "                        try:\n",
    "                            ab['@abstract-type']\n",
    "                        except:\n",
    "                            continue\n",
    "\n",
    "                        if ab['@abstract-type'] == 'author-highlights':\n",
    "\n",
    "                            if type(ab['p']) == dict:\n",
    "                                resumo = ab['p']['#text']\n",
    "                            elif type(ab['p']) == list:\n",
    "                                resumo = ''\n",
    "                                for p in ab['p']:\n",
    "                                    try:\n",
    "                                        p['#text']\n",
    "                                    except:\n",
    "                                        continue\n",
    "                                    resumo = \"%s\\n%s\" % (resumo, p['#text'])\n",
    "                else:\n",
    "                    resumo = ''\n",
    "            except Exception as e:\n",
    "                resumo = ''\n",
    "                print(e.__class__.__name__,e)\n",
    "                if debug:\n",
    "                    ipdb.set_trace()\n",
    "                    \n",
    "            ### Fim da grosseria\n",
    "                    \n",
    "            documento = {}\n",
    "            documento['resumo'] = resumo\n",
    "            documento['html_url'] = \"%s%s\" % (self._article_url, pmc_id)\n",
    "            documento['autores'] = \",\".join(authors)\n",
    "            documento['doi'] = doi\n",
    "            documento['palavras_chaves'] = \",\".join(palavras_chave)\n",
    "            documento['titulo'] = title\n",
    "\n",
    "\n",
    "            append(documento)\n",
    "            \n",
    "        print('{:15s}{:6.3f}'.format(\"fetch\",time.time() - t_04))\n",
    "\n",
    "        return documentos\n",
    "\n",
    "\n",
    "class PubMed_Searcher(NCBI_Searcher):\n",
    "    \"\"\"Realiza pesquisas na base PubMed.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def _fields(self):\n",
    "        return ['Text Words']\n",
    "\n",
    "    @property\n",
    "    def _db(self):\n",
    "        return 'pubmed'\n",
    "\n",
    "    @property\n",
    "    def _sort_order(self):\n",
    "        return ''\n",
    "\n",
    "    @property\n",
    "    def _article_url(self):\n",
    "        return \"https://www.ncbi.nlm.nih.gov/pubmed/\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_data(p_art):\n",
    "        \"\"\"Vasculha o XML (um <PubmedArticle>) para encontrar a data de publicação\n",
    "        Se for encontrada uma data válida, retorna um datetime.\n",
    "        Se não, retorna uma string, que espera-se que contenha uma informação de data\"\"\"\n",
    "\n",
    "        if hasattr(p_art.PubDate.Year, \"text\"):\n",
    "            ano = p_art.PubDate.Year.text\n",
    "        elif hasattr(p_art.PubDate.MedlineDate, \"text\"):\n",
    "            ano = p_art.PubDate.MedlineDate.text[:8]\n",
    "\n",
    "        try:\n",
    "            data_pub_string = \"%s %s\" % (ano, NCBI_Searcher.deepgetter(p_art, 'PubDate.Month.text', default='Jan'))\n",
    "            data = datetime.strptime(data_pub_string, \"%Y %b\").date()\n",
    "        except:\n",
    "            try:\n",
    "                data_pub_string = \"%s %s\" % (ano, NCBI_Searcher.deepgetter(p_art, 'PubDate.Month.text', default='Jan'))\n",
    "                data = datetime.strptime(data_pub_string, \"%Y %m\").date()\n",
    "            except:\n",
    "                data = str(p_art.PubDate.text)\n",
    "\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_unique_id(p_art):\n",
    "        \"\"\"Vascula o XML (um <PubmedArticle>) para encontrar o ID único do artigo.\n",
    "        Se nao tiver DOI presente no XML, coloca o ID que tiver (esperado que seja o PubMed ID)\"\"\"\n",
    "\n",
    "        try:\n",
    "            unique_id = p_art.findAll(\"ArticleId\", {\"IdType\": \"doi\"})[0].text\n",
    "        except:\n",
    "            unique_id = p_art.findAll(\"ArticleId\")[0]\n",
    "            unique_id = \"%s%s\" % (unique_id['IdType'], unique_id.text)\n",
    "\n",
    "        return unique_id\n",
    "\n",
    "    def _get_article_metadata(self, *args):\n",
    "        id_list = ','.join([str(x) for x in args])\n",
    "\n",
    "        payload = {\"id\": id_list, \"db\": self._db, \"retmode\": \"xml\"}\n",
    "        payload.update(self.ncbi_register)\n",
    "        url = \"%s?%s\" % (self.fetch_url, urlencode(payload))\n",
    "\n",
    "        print(\"URL META: %s\" % url)\n",
    "\n",
    "        t_03 = time.time()\n",
    "        soup = bsoup(requests.get(url).content, \"xml\")\n",
    "        print('{:15s}{:6.3f}'.format(\"parse\",time.time() - t_03))\n",
    "\n",
    "        pubmed_articles = soup.findAll('PubmedArticle')\n",
    "\n",
    "        documentos = []\n",
    "        append = documentos.append\n",
    "\n",
    "        for p_art in pubmed_articles:\n",
    "            authors = [\"%s %s\" % (a.ForeName.text, a.LastName.text) for a in p_art.findAll(\"Author\")]\n",
    "            keywords = [k.text for k in p_art.findAll(\"Keyword\")]\n",
    "\n",
    "            documento = {}\n",
    "            documento['resumo'] = getattr(p_art.AbstractText, 'text', ' - ')\n",
    "            documento['html_url'] = \"%s%s\" % (self._article_url, p_art.PMID.text)\n",
    "            documento['autores'] = \",\".join(authors)\n",
    "            documento['doi'] = self._get_unique_id(p_art)\n",
    "            documento['palavras_chaves'] = \",\".join(keywords)\n",
    "            documento['titulo'] = p_art.ArticleTitle.text\n",
    "            data = self._get_data(p_art)\n",
    "            if type(data) == str:\n",
    "                documento['resumo'] = \"%s\\n%s\" % (data, documento['resumo'])\n",
    "            else:\n",
    "                documento['data'] = self._get_data(p_art)\n",
    "\n",
    "            append(documento)\n",
    "\n",
    "        return documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artigos encontrados:  223\n",
      "URL META: https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?id=5703994%2C5552265%2C5431941%2C5493079%2C5749823%2C4429500%2C4405523%2C4831781%2C4364461%2C4589532%2C3422834%2C2846872%2C5070523%2C4672312%2C4063215%2C5751801%2C3733825%2C4896250%2C3079734%2C5001212&db=pmc&retmode=xml&tool=Atena&email=ddddiegolima%40gmail.com\n"
     ]
    }
   ],
   "source": [
    "# termos de pesquisa relacionados a tecnologia\n",
    "technology_queryterms = [\n",
    "    'machine learning', 'deep learning', 'artificial intelligence', \n",
    "    'neural network', 'scoring system'\n",
    "]\n",
    "\n",
    "# termos de pesquisa relacionados a area da saude\n",
    "health_queryterms = [\n",
    "    'coronary artery disease', 'chest pain', 'heart disease', 'MACE', \n",
    "    'Acute Cardiac Complications'\n",
    "]\n",
    "\n",
    "queryterms = [technology_queryterms, health_queryterms]\n",
    "\n",
    "scimago_journals = [\"Journal of the American College of Cardiology\",\"Circulation\",\"European Heart Journal\"\n",
    "                    ,\"Circulation Research\",\"Nature Biotechnology\",\"Current Opinion in Biotechnology\"\n",
    "                   ,\"Annual Review of Biomedical Engineering\", \"Circulation: Cardiovascular Interventions\"]\n",
    "eigenfactor_journals = [\"Medical image Analysis\",\"Biomaterials\",\"Acta Biomaterialia\",\"Physics in medicine and biology\",\n",
    "                        \"IEEE TRANSACTIONS ON MEDICAL IMAGING\",\"COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE \"\n",
    "                       ,\"INTERNATIONAL JOURNAL OF CARDIOLOGY\", \"CARDIOVASCULAR RESEARCH\", \"HEART RHYTHM\"\n",
    "                       ,\"EUROPEAN JOURNAL OF CARDIO-THORACIC SURGERY\",\"JACC-Cardiovascular Interventions\"\n",
    "                        ,\"JOURNAL OF MOLECULAR AND CELLULAR CARDIOLOGY \", \"JACC-Cardiovascular Imaging \"\n",
    "                        ,\"Circulation-Heart Failure\", \"EUROPEAN JOURNAL OF HEART FAILURE\", \"EUROPACE\"\n",
    "                       ,\"CATHETERIZATION AND CARDIOVASCULAR INTERVENTIONS\", \"Journal of the American Heart Association\"\n",
    "                       ,\"JOURNAL OF THE AMERICAN SOCIETY OF ECHOCARDIOGRAPHY\", \"Circulation-Cardiovascular Imaging\"]\n",
    "\n",
    "journal = [\"BioMedical Engineering OnLine\",\n",
    "           \"Biomedical Engineering\"] + scimago_journals + eigenfactor_journals\n",
    "\n",
    "\n",
    "queryterms = [['machine learning'], ['ck mb']]\n",
    "r = PMC_Searcher().search(queryterms=queryterms, max_records=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response        2.058\n",
      "Artigos encontrados:  223\n",
      "URL META: https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?id=5703994%2C5552265%2C5431941%2C5493079%2C5749823%2C4429500%2C4405523%2C4831781%2C4364461%2C4589532%2C3422834%2C2846872%2C5070523%2C4672312%2C4063215%2C5751801%2C3733825%2C4896250%2C3079734%2C5001212&db=pmc&retmode=xml&tool=Atena&email=ddddiegolima%40gmail.com\n",
      "response_M     14.294\n",
      "parse           3.058\n",
      "fetch          21.235\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Developing a Machine Learning ',\n",
       " 'Cardiac Function Improvement a',\n",
       " 'Precision Radiology: Predictin',\n",
       " '36th International Symposium o',\n",
       " 'Integrated genetic and epigene',\n",
       " 'Abstracts from the 37th Annual',\n",
       " 'Abstracts from the 38th Annual',\n",
       " 'Cardiac Troponin Is a Predicto',\n",
       " 'Abstracts for the 15th Interna',\n",
       " 'Programmable bio-nano-chip sys',\n",
       " 'Feature engineering combined w',\n",
       " 'A study of health effects of l',\n",
       " 'Learning statistical models of',\n",
       " 'Poster Session III\\nWednesday, ',\n",
       " 'Identifying Adverse Drug Event',\n",
       " 'Abstracts from the 9th Biennia',\n",
       " 'MUNDUS project: MUltimodal Neu',\n",
       " 'XXIV World Allergy Congress 20',\n",
       " 'Multiplexed Immunoassay Panel ',\n",
       " '25th Annual Computational Neur']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_01 = PMC_Searcher().search(queryterms=queryterms, max_records=20)\n",
    "[a['titulo'][:30] for a in r_01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response        0.843\n",
      "Artigos encontrados:  223\n",
      "URL META: https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?id=5703994%2C5552265%2C5431941%2C5493079%2C5749823%2C4429500%2C4405523%2C4831781%2C4364461%2C4589532%2C3422834%2C2846872%2C5070523%2C4672312%2C4063215%2C5751801%2C3733825%2C4896250%2C3079734%2C5001212%2C4669991%2C4261149%2C4672311%2C4943498%2C4672310%2C4389287%2C3654146%2C5592441%2C3204938%2C5374552%2C4869311%2C5042923%2C4871254%2C3639733%2C3751474%2C4212306%2C4261147%2C5149586%2C484364%2C5350766&db=pmc&retmode=xml&tool=Atena&email=ddddiegolima%40gmail.com\n",
      "response_M     46.773\n",
      "parse           9.446\n",
      "fetch          120.787\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Developing a Machine Learning ',\n",
       " 'Cardiac Function Improvement a',\n",
       " 'Precision Radiology: Predictin',\n",
       " '36th International Symposium o',\n",
       " 'Integrated genetic and epigene',\n",
       " 'Abstracts from the 37th Annual',\n",
       " 'Abstracts from the 38th Annual',\n",
       " 'Cardiac Troponin Is a Predicto',\n",
       " 'Abstracts for the 15th Interna',\n",
       " 'Programmable bio-nano-chip sys',\n",
       " 'Feature engineering combined w',\n",
       " 'A study of health effects of l',\n",
       " 'Learning statistical models of',\n",
       " 'Poster Session III\\nWednesday, ',\n",
       " 'Identifying Adverse Drug Event',\n",
       " 'Abstracts from the 9th Biennia',\n",
       " 'MUNDUS project: MUltimodal Neu',\n",
       " 'XXIV World Allergy Congress 20',\n",
       " 'Multiplexed Immunoassay Panel ',\n",
       " '25th Annual Computational Neur',\n",
       " 'UEG Week 2015 Poster Presentat',\n",
       " 'Poster Session III\\nWednesday, ',\n",
       " 'Poster Session II\\nTuesday, Dec',\n",
       " 'Proceedings of the 3rd IPLeiri',\n",
       " 'Poster Session I\\nMonday, Decem',\n",
       " 'High Field Small Animal Magnet',\n",
       " 'Abstracts from the 36th Annual',\n",
       " '26th Annual Computational Neur',\n",
       " 'Profiles and Majority Voting-B',\n",
       " '37th International Symposium o',\n",
       " 'Perfusion Pressure Cerebral In',\n",
       " 'ESICM LIVES 2016: part two',\n",
       " 'Choline metabolism-based molec',\n",
       " '2nd European Headache and Migr',\n",
       " 'Development and evaluation of ',\n",
       " 'UEG Week 2014 Poster Presentat',\n",
       " 'Poster Session I\\nMonday, Decem',\n",
       " 'ACNP 55th Annual Meeting: Post',\n",
       " 'POSTERS: On display in the Exh',\n",
       " 'ACTS Abstracts']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_02 = PMC_Searcher().search(queryterms=queryterms, max_records=40)\n",
    "[a['titulo'][:30] for a in r_02]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLIRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<?xml version=\"1.0\" ?>\\n<!DOCTYPE pmc-articleset PUBLIC \"-//NLM//DTD ARTICLE SET 2.0//EN\" \"https://dt'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?id=5703994%2C5552265%2C5431941%2C5493079%2C5749823%2C4429500%2C4405523%2C4831781%2C4364461%2C4589532%2C3422834%2C2846872%2C5070523%2C4672312%2C4063215%2C5751801%2C3733825%2C4896250%2C3079734%2C5001212&db=pmc&retmode=xml&tool=Atena&email=ddddiegolima%40gmail.com\"\n",
    "xml = requests.get(url).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = json.loads(json.dumps(xmltodict.parse(xml)))\n",
    "articles = d['pmc-articleset']['article']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError 'kwd-group'\n",
      "--Return--\n",
      "None\n",
      "> \u001b[0;32m<ipython-input-199-5c5df23bbbf5>\u001b[0m(31)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     30 \u001b[0;31m    \u001b[0;32mif\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 31 \u001b[0;31m        \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     32 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> continue\n"
     ]
    }
   ],
   "source": [
    "import ipdb\n",
    "i = 0\n",
    "debug = True\n",
    "### TITULO\n",
    "try:\n",
    "    title = articles[i]['front']['article-meta']['title-group']['article-title']\n",
    "except Exception as e:\n",
    "    print(e.__class__.__name__,e)\n",
    "    if debug:\n",
    "        ipdb.set_trace()\n",
    "        \n",
    "### AUTORES\n",
    "try:\n",
    "    authors = []\n",
    "    for contrib in articles[i]['front']['article-meta']['contrib-group']['contrib']:\n",
    "        try:\n",
    "            authors.append(\"%s %s\" % (contrib['name']['given-names'], contrib['name']['surname']))\n",
    "        except:\n",
    "            pass\n",
    "except Exception as e:\n",
    "    print(e.__class__.__name__,e)\n",
    "    if debug:\n",
    "        ipdb.set_trace()\n",
    "\n",
    "### PALAVRAS CHAVE \n",
    "try:    \n",
    "    palavras_chave = [k if type(k)==str else k['#text']for k in articles[i]['front']['article-meta']['kwd-group']['kwd']]\n",
    "except Exception as e:\n",
    "    print(e.__class__.__name__,e)\n",
    "    if debug:\n",
    "        ipdb.set_trace()\n",
    "    \n",
    "### DOI / IDs\n",
    "try:\n",
    "    doi = [id['#text'] for id in articles[i]['front']['article-meta']['article-id'] if  id['@pub-id-type'] == 'doi'][-1] or ''\n",
    "    pmc_id = [id['#text'] for id in articles[i]['front']['article-meta']['article-id'] if  id['@pub-id-type'] == 'pmc'][-1] or ''\n",
    "except Exception as e:\n",
    "    print(e.__class__.__name__,e)\n",
    "    if debug:\n",
    "        ipdb.set_trace()\n",
    "\n",
    "### Abstract\n",
    "try:\n",
    "    abstract = articles[i]['front']['article-meta']['abstract']\n",
    "    if type(abstract) == dict:\n",
    "        resumo = abstract['p']['#text']\n",
    "    elif type(abstract) == list:\n",
    "        for ab in abstract:\n",
    "            try:\n",
    "                ab['@abstract-type']\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            if ab['@abstract-type'] == 'author-highlights':\n",
    "                \n",
    "                if type(ab['p']) == dict:\n",
    "                    resumo = ab['p']['#text']\n",
    "                elif type(ab['p']) == list:\n",
    "                    resumo = ''\n",
    "                    for p in ab['p']:\n",
    "                        try:\n",
    "                            p['#text']\n",
    "                        except:\n",
    "                            continue\n",
    "                        resumo = \"%s\\n%s\" % (resumo, p['#text'])\n",
    "    else:\n",
    "        resumo = ''\n",
    "except Exception as e:\n",
    "    print(e.__class__.__name__,e)\n",
    "    if debug:\n",
    "        ipdb.set_trace()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (title, '\\n')\n",
    "print (authors, '\\n')\n",
    "print (palavras_chave, '\\n')\n",
    "print (doi, '\\n')\n",
    "print (pmc_id, '\\n')\n",
    "print (resumo, '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response        1.031\n",
      "Artigos encontrados:  223\n",
      "URL META: https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?id=5703994%2C5552265%2C5431941%2C5493079%2C5749823%2C4429500%2C4405523%2C4831781%2C4364461%2C4589532%2C3422834%2C2846872%2C5070523%2C4672312%2C4063215%2C5751801%2C3733825%2C4896250%2C3079734%2C5001212&db=pmc&retmode=xml&tool=Atena&email=ddddiegolima%40gmail.com\n",
      "response_M     14.275\n",
      "parse           1.196\n",
      "KeyError 'kwd-group'\n",
      "> \u001b[0;32m<ipython-input-209-a0bb5af0b834>\u001b[0m(323)\u001b[0;36m_get_article_metadata\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    322 \u001b[0;31m            \u001b[0;31m### DOI / IDs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 323 \u001b[0;31m            \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    324 \u001b[0;31m                \u001b[0mdoi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'#text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marticle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'front'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'article-meta'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'article-id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m  \u001b[0mid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'@pub-id-type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'doi'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> continue\n",
      "KeyError 'kwd-group'\n",
      "> \u001b[0;32m<ipython-input-209-a0bb5af0b834>\u001b[0m(320)\u001b[0;36m_get_article_metadata\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    319 \u001b[0;31m                \u001b[0;32mif\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 320 \u001b[0;31m                    \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    321 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> continue\n",
      "TypeError string indices must be integers\n",
      "> \u001b[0;32m<ipython-input-209-a0bb5af0b834>\u001b[0m(367)\u001b[0;36m_get_article_metadata\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    366 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 367 \u001b[0;31m            \u001b[0mdocumento\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    368 \u001b[0;31m            \u001b[0mdocumento\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'resumo'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresumo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "r_a = PMC_Searcher().search(queryterms=queryterms, max_records=20)\n",
    "[a['titulo'][:30] for a in r_a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
